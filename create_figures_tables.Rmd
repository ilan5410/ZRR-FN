---
title: "script"
output:
  pdf_document: default
  html_document: default
date: "2024-10-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Define the paths
```{r }
rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO copy/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO copy/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")
keep_vars<- c(path_data, path_out, path_figures, path_tables)

main_path <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO copy/"
raw_data_path <- paste0(path_data, "raw data/")
processed_data_path <-  paste0(path_data,  "processed data/")


# To print the latex table in a specific way
options("modelsummary_format_numeric_latex" = "plain")

```


# Libraries
```{r }
pacman::p_load(
readxl, dplyr, tidyr, ggplot2, sf, stargazer, AER, broom, ggrepel, sf, viridis, stringr, ragg, fixest, sandwich, zoo, readr, rdd, rddtools, magrittr, rdrobust, estimatr, knitr, modelsummary, rddensity, gridExtra, rnaturalearth, rnaturalearthdata, forcats, cowplot, plm, grid, patchwork, reshape2,
  kableExtra, caret, htetree, glmnet, rpart, randomForest, purrr, devtools,  SuperLearner, xgboost, psych, MatchIt, multiwayvcov, clubSandwich, patchwork

  )
```
# Checks

## Check that the canton and treatment overlap
```{r }
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```

```{r }
print("Number of cantons:")
print(length(unique(df_rdd$canton)))

# Check if each canton has only one unique value for `z`
canton_check <- df_rdd %>%
  group_by(canton) %>%
  summarize(unique_z_values = n_distinct(z)) %>%
  filter(unique_z_values > 1)

# Display the result
if (nrow(canton_check) == 0) {
  print("No overlap: all municipalities within each canton are consistently treated or not treated.")
} else {
  print("Overlap detected: some cantons have both treated and non-treated municipalities.")
  print(canton_check)
}
```

# Figures
## Figure 1: Evolution of FN Vote Share (1988-2022) - Presidential/Legislative Elections in France

```{r }
if (file.exists(paste0(processed_data_path, "main.RData"))) {
  load(paste0(processed_data_path, "main.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }
# Prepare presidential vote share
df_pres <- dfZRR %>%
  filter(year == 2005) %>%
  select(codecommune, FN1988, FN1995, FN2002, FN2007, FN2012, FN2017, FN2022) %>%
  distinct() %>%
  mutate(across(everything(), as.numeric)) %>%
  pivot_longer(cols = starts_with("FN"), names_to = "year", values_to = "vote_share") %>%
  mutate(year = as.numeric(gsub("FN", "", year)))

df_pres_summary <- df_pres %>%
  group_by(year) %>%
  summarize(mean_vote_share = mean(vote_share, na.rm = TRUE),
            sd_vote_share = sd(vote_share, na.rm = TRUE)) %>%
  mutate(type = "Presidential")

# Manually add legislative vote shares
df_leg_summary <- tibble(
  year = c(1988, 1993, 1997, 2002, 2007, 2012, 2017, 2022),
  mean_vote_share = c(0.097, 0.124, 0.150, 0.113, 0.043, 0.136, 0.132, 0.187),
  sd_vote_share = NA,
  type = "Legislative"
)

# Combine the two datasets
df_summary_combined <- bind_rows(df_pres_summary, df_leg_summary)

# Plot both
plot <- ggplot(df_summary_combined, aes(x = year, y = mean_vote_share, color = type, linetype = type)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  scale_x_continuous(breaks = sort(unique(df_summary_combined$year))) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_color_manual(values = c("Presidential" = "blue", "Legislative" = "red")) +
  scale_fill_manual(values = c("Presidential" = "blue", "Legislative" = NA)) +
  labs(title = "Evolution of FN Vote Share (1988–2022)",
       subtitle = "Presidential and Legislative Elections in France",
       x = "Year", y = "Mean Vote Share (%)",
       color = "Election Type", linetype = "Election Type", fill = "Election Type") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14, face = "bold"),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 13),
    legend.text = element_text(size = 12)
  )

print(plot)

# Save the plot
ggsave(filename = file.path(path_figures, "figure1.png"), plot = plot, width = 8, height = 6, dpi = 300)


rm(df_leg_summary, df_pres, df_pres_summary, plot, df_merged, dfcontrols, dfPop, dfPopLong, dfRevenue, create_plotsFN)

```

 
## Figure 2: FN by typology
```{r }
if (file.exists(paste0(path_data, "FN_growth.RData"))) {
  load(paste0(path_data, "FN_growth.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }

# Prepare the data
df <- dfZRR1995Controls %>%
  select(year, FN, typologie) %>%
  #filter(year %in% c(1988, 1995, 2002, 2012)) %>%
  mutate(
    typologie_group = case_when(
      typologie %in% c("rural autonome peu dense", 
                       "rural autonome très peu dense",
                       "rural sous faible influence d'un pôle",
                       "rural sous forte influence d'un pôle") ~ "Rural",
      typologie %in% c("urbain densité intermédiaire") ~ "Intermediary",
      typologie %in% c("urbain dense") ~ "Urban",
      TRUE ~ NA_character_
    )
  )

# Compute means
df_summary <- df %>%
  group_by(year, typologie_group) %>%
  summarise(mean_FN = mean(FN, na.rm = TRUE), .groups = "drop")

# Order categories
df_summary <- df_summary %>%
  mutate(
    typologie_group = fct_relevel(typologie_group, "Rural", "Intermediary", "Urban"),
    year = as.factor(year)
  )

# Plot grouped bars
plot <- ggplot(df_summary, aes(x = year, y = mean_FN, fill = typologie_group)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(
    aes(label = paste0(round(mean_FN * 100, 0), "%")),
    position = position_dodge(width = 0.8),
    vjust = -0.5,
    size = 3.5
  ) +
  labs(
    x = "Year",
    y = "Mean FN Vote Share",
    fill = "Typologie Group"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 13, face = "bold"),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

print(plot)

# Save plot
ggsave(filename = file.path(path_figures, "FN_typology.png"), plot = plot, width = 10, height = 6)

```

```{r }
rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")


```


## Figure 2_quint:  FN vs. variable of choice
```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }

create_plotsFN <- function(dfReg, x_var, var_lab, years, num_bins = 50, delta = TRUE) {
  
  # Filter and select relevant data
  dfReg_filtered <- dfReg %>%
    filter(year %in% years) %>%
    select(codecommune, year, x = {{x_var}}, FN) %>%
    distinct()  # Removes duplicates
  
  # Pivot x and FN values to wide format by year
  dfReg_wide <- dfReg_filtered %>%
    pivot_wider(names_from = year, values_from = c(x, FN), names_sep = "")
  
  
  # Calculate deltas for x and FN across consecutive years if delta is TRUE
  sorted_years <- sort(as.numeric(years))
  if (delta) {
    for (i in 2:length(sorted_years)) {
      current_year <- sorted_years[i]
      previous_year <- sorted_years[i - 1]
      
      dfReg_wide[[paste0("Delta_x_", current_year)]] <- dfReg_wide[[paste0("x", current_year)]] - dfReg_wide[[paste0("x", previous_year)]]
      dfReg_wide[[paste0("Delta_FN_", current_year)]] <- dfReg_wide[[paste0("FN", current_year)]] - dfReg_wide[[paste0("FN", previous_year)]]
    }
  }

  # Update variable prefixes for delta or non-delta conditions
  x_var_prefix <- if (delta) "Delta_x_" else "x"
  y_var_prefix <- if (delta) "Delta_FN_" else "FN"
  years_to_plot <- if (delta) sorted_years[-1] else sorted_years

  dfReg_wide <- dfReg_wide %>% 
  filter_if(~is.numeric(.), all_vars(!is.infinite(.)))
  
  # Initialize plot list and panel labels
  plots <- list()
  panel_labels <- paste0("Panel ", LETTERS[1:length(years_to_plot)], ": ")
  
  # Generate plots
  for (i in seq_along(years_to_plot)) {
    year <- years_to_plot[i]
    xVar <- paste0(x_var_prefix, year)
    yVar <- paste0(y_var_prefix, year)
    
    # Ensure numeric conversion
    dfReg_wide[[xVar]] <- as.numeric(dfReg_wide[[xVar]])
    dfReg_wide[[yVar]] <- as.numeric(dfReg_wide[[yVar]])
    
    # Calculate correlation coefficient
    correlation <- cor(dfReg_wide[[xVar]], dfReg_wide[[yVar]], use = "complete.obs", method = "pearson")
    
    # Create bins and calculate bin centers and means
    bin_var <- paste0(xVar, "_bins")
    df_plot <- dfReg_wide %>%
      mutate(!!bin_var := cut(!!sym(xVar), breaks = num_bins, labels = FALSE)) %>%
      group_by(!!sym(bin_var)) %>%
      summarise(
        bin_center = mean(!!sym(xVar), na.rm = TRUE),
        mean_y = mean(!!sym(yVar), na.rm = TRUE),
        count = n(),
        .groups = 'drop'
      )
    
    # Plot for the current year or year pair
    t <- if (delta) "FN Vote Share (Delta)" else "FN Vote Share"
    p <- ggplot(df_plot, aes(x = bin_center, y = mean_y)) +
      geom_point(alpha = 0.5) +
      geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "black") +
      labs(title = paste0(panel_labels[i], year, " - ", " (r = ", round(correlation, 2), ")")) +
      theme_minimal() +
      theme(axis.title = element_blank())
    
    plots[[i]] <- p
  }

  # Arrange plots in a grid with centered axis labels
  n_cols <- ceiling(sqrt(length(plots)))
  n_rows <- ceiling(length(plots) / n_cols)
  
  grid.arrange(
    arrangeGrob(grobs = plots, nrow = n_rows, ncol = n_cols),
    bottom = textGrob(var_lab, gp = gpar(fontsize = 14)),
    left = textGrob(t, rot = 90, gp = gpar(fontsize = 14))
  )
}

```

```{r }
df <- dfZRRControls %>% 
  mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>%
  left_join(dfOutcomeLong %>% 
              mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>% 
              unique(), by=c("codecommune", "year")
            ) %>%
  select(codecommune, year, pop, FN) %>% # choose variable here. ---- min_distance_to_agglo, 
  drop_na() %>% 
  filter_all(all_vars(!is.infinite(.)))


df$logpop <- log(df$pop+1)


create_plotsFN(
  df, 
  x_var = "logpop", 
  var_lab = "Population size (log)",
  years = c("1988", "1995", "2007", "2012"),
  num_bins = 20,
  delta=FALSE
)


# Export the combined plot
# file_path <- paste0(path_figures, "deltaRev_vs_deltaFN.png")
# ggsave(file_path, combined_plot, width = 10, height = 8)



rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")


```





## Figure 2_sixt:  delta FN vs. variable of choice
```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }

create_plotsFN <- function(dfReg, x_var, var_lab, years, num_bins = 50, delta = TRUE) {
  
  # Filter and select relevant data
  dfReg_filtered <- dfReg %>%
    filter(year %in% years) %>%
    select(codecommune, year, x = {{x_var}}, FN) %>%
    distinct()  # Removes duplicates
  
  # Pivot x and FN values to wide format by year
  dfReg_wide <- dfReg_filtered %>%
    pivot_wider(names_from = year, values_from = c(x, FN), names_sep = "")
  
  # Calculate deltas for FN across consecutive years 
  sorted_years <- sort(as.numeric(years))
  
  for (i in 2:length(sorted_years)) {
    current_year <- sorted_years[i]
    previous_year <- sorted_years[i - 1]
    
    dfReg_wide[[paste0("Delta_FN_", current_year)]] <- dfReg_wide[[paste0("FN", current_year)]] - dfReg_wide[[paste0("FN", previous_year)]]
  }

  # Filter infinite values
  dfReg_wide <- dfReg_wide %>% 
    filter_if(is.numeric, all_vars(!is.infinite(.)))

  # Initialize plot list and panel labels
  plots <- list()
  panel_labels <- paste0("Panel ", LETTERS[1:(length(sorted_years) - 1)], ": ")
  
  # Generate plots for each year interval
  for (i in 1:(length(sorted_years) - 1)) {
    previous_year <- sorted_years[i]
    current_year <- sorted_years[i + 1]
    
    xVar <- paste0("x", previous_year)  # x corresponds to the previous year
    yVar <- paste0("Delta_FN_", current_year)  # Delta_FN corresponds to the current year
    
    # Ensure numeric conversion
    dfReg_wide[[xVar]] <- as.numeric(dfReg_wide[[xVar]])
    dfReg_wide[[yVar]] <- as.numeric(dfReg_wide[[yVar]])
    
    # Calculate correlation coefficient
    correlation <- cor(dfReg_wide[[xVar]], dfReg_wide[[yVar]], use = "complete.obs", method = "pearson")
    
    # Create bins and calculate bin centers and means
    bin_var <- paste0(xVar, "_bins")
    df_plot <- dfReg_wide %>%
      mutate(!!bin_var := cut(!!sym(xVar), breaks = num_bins, labels = FALSE)) %>%
      group_by(!!sym(bin_var)) %>%
      summarise(
        bin_center = mean(!!sym(xVar), na.rm = TRUE),
        mean_y = mean(!!sym(yVar), na.rm = TRUE),
        count = n(),
        .groups = 'drop'
      )
    
    # Plot for the current year pair
    t <- "FN Vote Share (Delta)"
    p <- ggplot(df_plot, aes(x = bin_center, y = mean_y)) +
      geom_point(alpha = 0.5) +
      geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "black") +
      labs(title = paste0(panel_labels[i], previous_year, "->", current_year, " (r = ", round(correlation, 2), ")")) +
      theme_minimal() +
      theme(axis.title = element_blank())
    
    plots[[i]] <- p
  }

  # Arrange plots in a grid with centered axis labels
  n_cols <- ceiling(sqrt(length(plots)))
  n_rows <- ceiling(length(plots) / n_cols)
  
  grid.arrange(
    arrangeGrob(grobs = plots, nrow = n_rows, ncol = n_cols),
    bottom = textGrob(var_lab, gp = gpar(fontsize = 14)),
    left = textGrob(t, rot = 90, gp = gpar(fontsize = 14))
  )
}

```

```{r }
df <- dfZRRControls %>% 
  mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>%
  left_join(dfOutcomeLong %>% 
              mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>% 
              unique(), by=c("codecommune", "year")
            ) %>%
  select(codecommune, year, pop, FN) %>% # choose variable here. ---- min_distance_to_agglo, 
  drop_na() %>% 
  filter_all(all_vars(!is.infinite(.)))


df$logpop <- log(df$pop+1)


create_plotsFN(
  df, 
  x_var = "logpop", 
  var_lab = "Population size (log) of the start year",
  years = c("1988", "2017"),
  num_bins = 20,
  delta=FALSE
)


# Export the combined plot
# file_path <- paste0(path_figures, "deltaRev_vs_deltaFN.png")
# ggsave(file_path, combined_plot, width = 10, height = 8)



rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")


```

## Figure 2_sept:  treatment and pop size
```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }

df <- dfZRRControls %>% 
  mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>%
  select(codecommune, year, pop, treatment) %>%
  filter(year==1988) %>% 
  drop_na() %>% 
  filter_all(all_vars(!is.infinite(.)))

df$logpop <- log(df$pop+1)

# Create density plot
ggplot(df, aes(x = logpop, fill = as.factor(treatment))) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("0" = "blue", "1" = "red"), labels = c("Control", "Treatment")) +
  labs(
    title = "Population Distribution by Treatment Status",
    x = "Population (log)",
    y = "Density",
    fill = "Treatment"
  ) +
  theme_minimal()


```

```{r }

# Export the  plot
# file_path <- paste0(path_figures, "deltaRev_vs_deltaFN.png")
# ggsave(file_path, combined_plot, width = 10, height = 8)



rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")


```

## Figure 2_eight:  delta FN, pop and treatment density
```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }

create_plotsFN_with_density <- function(dfReg, x_var, var_lab, years, num_bins = 50, delta = TRUE) {
  
  # Filter and select relevant data
  dfReg_filtered <- dfReg %>%
    filter(year %in% years) %>%
    select(codecommune, year, x = {{x_var}}, FN, treatment) %>%
    distinct()  # Removes duplicates
  
  # Pivot x, FN, and treatment values to wide format by year
  dfReg_wide <- dfReg_filtered %>%
    pivot_wider(names_from = year, values_from = c(x, FN, treatment), names_sep = "")
  
  # Calculate deltas for FN across consecutive years 
  sorted_years <- sort(as.numeric(years))
  
  for (i in 2:length(sorted_years)) {
    current_year <- sorted_years[i]
    previous_year <- sorted_years[i - 1]
    
    dfReg_wide[[paste0("Delta_FN_", current_year)]] <- dfReg_wide[[paste0("FN", current_year)]] - dfReg_wide[[paste0("FN", previous_year)]]
  }

  # Filter infinite values
  dfReg_wide <- dfReg_wide %>% 
    filter_if(is.numeric, all_vars(!is.infinite(.)))

  # Initialize plot list and panel labels
  plots <- list()
  panel_labels <- paste0("Panel ", LETTERS[1:(length(sorted_years) - 1)], ": ")
  
  # Generate plots for each year interval
  for (i in 1:(length(sorted_years) - 1)) {
    previous_year <- sorted_years[i]
    current_year <- sorted_years[i + 1]
    
    xVar <- paste0("x", previous_year)  # x corresponds to the previous year
    yVar <- paste0("Delta_FN_", current_year)  # Delta_FN corresponds to the current year
    treatmentVar <- paste0("treatment", previous_year)  # Treatment corresponds to the start year of the interval
    
    # Ensure numeric conversion
    dfReg_wide[[xVar]] <- as.numeric(dfReg_wide[[xVar]])
    dfReg_wide[[yVar]] <- as.numeric(dfReg_wide[[yVar]])
    
    # Calculate correlation coefficient
    correlation <- cor(dfReg_wide[[xVar]], dfReg_wide[[yVar]], use = "complete.obs", method = "pearson")
    
    # Create bins and calculate bin centers, means, and treatment density
    bin_var <- paste0(xVar, "_bins")
    df_plot <- dfReg_wide %>%
      mutate(!!bin_var := cut(!!sym(xVar), breaks = num_bins, labels = FALSE)) %>%
      group_by(!!sym(bin_var)) %>%
      summarise(
        bin_center = mean(!!sym(xVar), na.rm = TRUE),
        mean_y = mean(!!sym(yVar), na.rm = TRUE),
        count = n(),
        treatment_density = mean(!!sym(treatmentVar), na.rm = TRUE),
        .groups = 'drop'
      )
    
    # Plot for the current year pair
    t <- "FN Vote Share (Delta)"
    p <- ggplot(df_plot, aes(x = bin_center)) +
      geom_point(aes(y = mean_y), alpha = 0.5, color = "blue") +
      geom_smooth(aes(y = mean_y), method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "black") +
      geom_line(aes(y = treatment_density * max(mean_y, na.rm = TRUE)), color = "red", linetype = "dashed") +
      labs(title = paste0(panel_labels[i], previous_year, "->", current_year, " (r = ", round(correlation, 2), ")")) +
      theme_minimal() +
      theme(axis.title = element_blank())
    
    plots[[i]] <- p
  }

  # Arrange plots in a grid with centered axis labels
  n_cols <- ceiling(sqrt(length(plots)))
  n_rows <- ceiling(length(plots) / n_cols)
  
  grid.arrange(
    arrangeGrob(grobs = plots, nrow = n_rows, ncol = n_cols),
    bottom = textGrob(var_lab, gp = gpar(fontsize = 14)),
    left = textGrob(t, rot = 90, gp = gpar(fontsize = 14))
  )
}

```

```{r }
df <- dfZRRControls %>% 
  mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>%
  left_join(dfOutcomeLong %>% 
              mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>% 
              unique(), by=c("codecommune", "year")
            ) %>%
  select(codecommune, year, year_treat, treatment, pop, FN) %>% # choose variable here. ---- min_distance_to_agglo, 
  drop_na() %>% 
  filter_all(all_vars(!is.infinite(.)))


df$logpop <- log(df$pop+1)


create_plotsFN_with_density(
  df, 
  x_var = "logpop", 
  var_lab = "Population size (log) of the start year",
  years = c("1988", "2017"),
  num_bins = 20,
  delta=FALSE
)


# Export the combined plot
# file_path <- paste0(path_figures, "deltaRev_vs_deltaFN.png")
# ggsave(file_path, combined_plot, width = 10, height = 8)



rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")


```



```{r }

plot_treatment_density <- function(dfReg, x_var, var_lab, num_bins = 50, start_year) {

  # Filter the data for the start year
  df_filtered <- dfReg %>%
    filter(year == start_year) %>%
    select(codecommune, x = {{x_var}}, treatment) %>%
    distinct()  # Remove duplicates
  
  # Create bins and calculate the proportion of treated municipalities
  bin_var <- "x_bins"
  df_plot <- df_filtered %>%
    mutate(!!bin_var := cut(x, breaks = num_bins, labels = FALSE)) %>%
    group_by(!!sym(bin_var)) %>%
    summarise(
      bin_center = mean(x, na.rm = TRUE),
      treatment_proportion = mean(treatment, na.rm = TRUE),
      count = n(),
      .groups = 'drop'
    )
  
  # Plot the proportion of treatment
  ggplot(df_plot, aes(x = bin_center, y = treatment_proportion)) +
    geom_bar(stat = "identity", alpha = 0.7) +
    labs(
      title = paste0("Proportion of Treated Municipalities (", start_year, ")"),
      x = var_lab,
      y = "Proportion Treated"
    ) +
    theme_minimal()
}

plot_treatment_density(
  df, 
  x_var = "logpop", 
  var_lab = "Population size (log)",
  start_year = "1988"
)

#hist(df$logpop)

```


```{r }

create_plotsFN_with_density <- function(dfReg, x_var, var_lab, years, num_bins = 50, delta = TRUE) {
  
  # Filter and select relevant data
  dfReg_filtered <- dfReg %>%
    filter(year %in% years) %>%
    select(codecommune, year, x = {{x_var}}, FN, treatment) %>%
    distinct()  # Removes duplicates
  
  # Pivot x, FN, and treatment values to wide format by year
  dfReg_wide <- dfReg_filtered %>%
    pivot_wider(names_from = year, values_from = c(x, FN, treatment), names_sep = "")
  
  # Calculate deltas for FN across consecutive years 
  sorted_years <- sort(as.numeric(years))
  
  for (i in 2:length(sorted_years)) {
    current_year <- sorted_years[i]
    previous_year <- sorted_years[i - 1]
    
    dfReg_wide[[paste0("Delta_FN_", current_year)]] <- dfReg_wide[[paste0("FN", current_year)]] - dfReg_wide[[paste0("FN", previous_year)]]
  }

  # Filter infinite values
  dfReg_wide <- dfReg_wide %>% 
    filter_if(is.numeric, all_vars(!is.infinite(.)))

  # Initialize plot list and panel labels
  plots <- list()
  panel_labels <- paste0("Panel ", LETTERS[1:(length(sorted_years) - 1)], ": ")
  
  # Generate plots for each year interval
  for (i in 1:(length(sorted_years) - 1)) {
    previous_year <- sorted_years[i]
    current_year <- sorted_years[i + 1]
    
    xVar <- paste0("x", previous_year)  # x corresponds to the previous year
    yVar <- paste0("Delta_FN_", current_year)  # Delta_FN corresponds to the current year
    treatmentVar <- paste0("treatment", previous_year)  # Treatment corresponds to the start year of the interval
    
    # Ensure numeric conversion
    dfReg_wide[[xVar]] <- as.numeric(dfReg_wide[[xVar]])
    dfReg_wide[[yVar]] <- as.numeric(dfReg_wide[[yVar]])
    
    # Calculate correlation coefficient
    correlation <- cor(dfReg_wide[[xVar]], dfReg_wide[[yVar]], use = "complete.obs", method = "pearson")
    
    # Create bins and calculate bin centers, means, and treatment density
    bin_var <- paste0(xVar, "_bins")
    df_plot <- dfReg_wide %>%
      mutate(!!bin_var := cut(!!sym(xVar), breaks = num_bins, labels = FALSE)) %>%
      group_by(!!sym(bin_var)) %>%
      summarise(
        bin_center = mean(!!sym(xVar), na.rm = TRUE),
        mean_y = mean(!!sym(yVar), na.rm = TRUE),
        count = n(),
        treatment_proportion = mean(!!sym(treatmentVar), na.rm = TRUE),
        .groups = 'drop'
      )
    
    # Scale the treatment proportion to match the secondary y-axis
    max_y <- max(df_plot$mean_y, na.rm = TRUE)
    df_plot <- df_plot %>%
      mutate(treatment_scaled = treatment_proportion * max_y)
    
    # Plot for the current year pair
    t <- "FN Vote Share (Delta)"
    p <- ggplot(df_plot, aes(x = bin_center)) +
      geom_point(aes(y = mean_y), alpha = 0.5, color = "blue") +
      geom_smooth(aes(y = mean_y), method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "black") +
      geom_line(aes(y = treatment_scaled), color = "red", linetype = "dashed") +
      scale_y_continuous(
        name = "FN Vote Share (Delta)",
        sec.axis = sec_axis(~ . / max_y, name = "Proportion Treated", labels = scales::percent)
      ) +
      labs(title = paste0(panel_labels[i], previous_year, "->", current_year, " (r = ", round(correlation, 2), ")")) +
      theme_minimal() +
      theme(axis.title.x = element_blank())
    
    plots[[i]] <- p
  }

  # Arrange plots in a grid with centered axis labels
  n_cols <- ceiling(sqrt(length(plots)))
  n_rows <- ceiling(length(plots) / n_cols)
  
  grid.arrange(
    arrangeGrob(grobs = plots, nrow = n_rows, ncol = n_cols),
    bottom = textGrob(var_lab, gp = gpar(fontsize = 14)),
    left = textGrob(t, rot = 90, gp = gpar(fontsize = 14))
  )
}

create_plotsFN_with_density(
  df, 
  x_var = "logpop", 
  var_lab = "Population size (log) of the start year",
  years = c("1988", "2017"),
  num_bins = 20,
  delta=FALSE
)

```
```{r }
rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")

```

## Figure 2_nine:  same with colored bins
```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}

df <- dfZRRControls %>% 
  mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>%
  left_join(dfOutcomeLong %>% 
              mutate(codecommune = sub("^0+", "", as.character(codecommune))) %>% 
              unique(), by=c("codecommune", "year")
            ) %>%
  select(codecommune, year, year_treat, treatment, pop, FN) %>% # choose variable here. ---- min_distance_to_agglo, 
  drop_na() %>% 
  filter_all(all_vars(!is.infinite(.))) %>%
  mutate(logpop = log(df$pop+1))



# Filter for data in the year 1988 and 2017
df_filtered <- df %>%
  filter(year %in% c(1988, 2017)) %>%
  group_by(codecommune) %>%
  summarise(
    logpop_1988 = logpop[year == 1988],
    delta_FN = FN[year == 2017] - FN[year == 1988],
    year_treat = unique(year_treat)
  ) %>%
  filter(!is.na(logpop_1988), !is.na(delta_FN)) # Remove NAs

# Create bins for logpop_1988
df_binned <- df_filtered %>%
  mutate(bin = cut(logpop_1988, breaks = 30, labels = FALSE))

# Assign colors based on year_treat
df_binned <- df_binned %>%
  mutate(color_group = case_when(
    year_treat == 1995 ~ "1995",
    year_treat > 2003 ~ "After 2003",
    TRUE ~ "Other"
  ))

# Plot the scatterplot
ggplot(df_binned, aes(x = logpop_1988, y = delta_FN, color = color_group)) +
  geom_point(alpha = 0.7) +  # Scatter points
  scale_color_manual(values = c("1995" = "red", "After 2003" = "blue", "Other" = "gray")) +
  labs(
    title = "Change in FN Support vs Log of Population (1988)",
    x = "Log(Population in 1988)",
    y = "Change in FN Support (1988 to 2017)",
    color = "Year Treat Group"
  ) +
  theme_minimal()
```

```{r }
# Define parameters
xVar <- "logpop_1988"
yVar <- "delta_FN"
treatmentVar <- "year_treat"
bin_var <- "bin"
num_bins <- 20

# Filter and prepare the data
df_filtered <- df %>%
  filter(year %in% c(1988, 2017)) %>%
  group_by(codecommune) %>%
  summarise(
    logpop_1988 = logpop[year == 1988],
    delta_FN = FN[year == 2017] - FN[year == 1988],
    year_treat = unique(year_treat),
    .groups = 'drop'
  ) %>%
  filter(!is.na(logpop_1988), !is.na(delta_FN)) # Remove NAs

# Add a treatment group identifier
df_filtered <- df_filtered %>%
  mutate(treatment_group = case_when(
    year_treat == 1995 ~ "Year Treat = 1995",
    year_treat > 2003 ~ "Year Treat > 2003",
    TRUE ~ "Never treated"
  ))



# Bin the data and calculate averages for each group
df_binned <- df_filtered %>%
  mutate(!!bin_var := cut(!!sym(xVar), breaks = num_bins, labels = FALSE)) %>%
  group_by(!!sym(bin_var), treatment_group) %>%
  summarise(
    bin_center = mean(!!sym(xVar), na.rm = TRUE),
    mean_y = mean(!!sym(yVar), na.rm = TRUE),
    count = n(),
    .groups = 'drop'
  )


# Plot all groups on the same graph
ggplot(df_binned, aes(x = bin_center, y = mean_y, color = treatment_group)) +
  geom_point(size = 2, alpha = 0.8) +
  # geom_line(aes(group = treatment_group), linewidth = 1) +  # Add lines connecting points for each group
  scale_color_manual(
    values = c(
      "Year Treat = 1995" = "#E63946",  # Red
      "Year Treat > 2003" = "#457B9D",  # Blue
      "Never treated" = "#A8A8A8"  # Gray
    )
  ) +
  labs(
    title = "Binned Average Change in FN Support vs Log of Population (1988)",
    subtitle = "Combined groups with treatment year differentiation",
    x = "Log(Population in 1988) - Bin Center",
    y = "Average Change in FN Support (1988 to 2017)",
    color = "Treatment Group"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.position = "top",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major = element_line(color = "gray80", linewidth = 0.5),
    panel.grid.minor = element_blank()
  )
```
```{r }
rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")

```

## Figure 3: Map of the municipalities in the ZRR program

```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}


dfMap <- dfZRRControls  %>%
  filter(year %in% list(1988) )  %>%
  mutate(treatment_status = case_when(
    year_treat == 1995 ~ "treat_1995",
    year_treat > 1995 ~ "after 2004",
    year_treat == 0 ~ "never treated"
  )) %>%
  select(codecommune, treatment_status, year_treat,treatment) 


# Load the shape file into a dataframe
dfShape <- st_read(paste0(path_data, "communes-20220101-shp/communes-20220101.shp")) %>%
  select(geometry, insee) %>%
  mutate(codecommune = sub("^0+", "", as.character(insee))) %>% select(-insee)

dfMap <- merge(dfMap, dfShape, on="codecommune") 
dfMap <- st_as_sf(dfMap)
dfMap <- dfMap %>%
  filter(as.numeric(codecommune) <= 96000) %>%
  mutate(treatment_status = replace_na(treatment_status, "never treated"))


# Plot the map with treatment status
p <- ggplot(dfMap) +
  geom_sf(aes(fill = treatment_status), color = NA) +
  scale_fill_manual(
    values = c(
      "treat_1995" = "black",
      "after 2004" = "grey30",
      "never treated" = "grey80"
    ),
    labels = c(
      "treat_1995" = "Treatment in 1995",
      "after 2004" = "Treatment after 2004",
      "never treated" = "Never Treated"
    )
  ) +
  labs(
    title = "Treatment Status of Counties",
    fill = "Treatment Status"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, size = 16)
  )

file_path <- paste0(path_figures, "treatmentMap.png")
ggsave(file_path, plot = p, device = "png", width = 10, height = 8)


rm(list = setdiff(ls(), keep_vars))

```




## Figure 4: FN vote share in the first round of the 1988 and 2002 presidential elections.

```{r }

# Preprocess data function
preprocess_data <- function(df, dfShape, var) {
  df <- df %>%
    rename(insee = codecommune) %>%
    select(var, "insee") %>%
    mutate(insee = as.character(insee)) %>%
    mutate(insee = sub("^0+", "", insee))
  
  dfShape <- dfShape %>%
    mutate(insee = as.character(insee)) %>%
    mutate(insee = sub("^0+", "", insee))
  
  merged_df <- df %>%
    inner_join(dfShape, by = "insee") %>%
    select(insee, all_of(var), geometry) %>%
    mutate_at(vars(one_of(var)), as.numeric) %>%
    filter_all(all_vars(!is.na(.))) %>%
    filter_if(is.numeric, all_vars(is.finite(.)))
  
  return(merged_df)
}


# Load the shape file into a dataframe
pathshp <- paste0(path_data, "communes-20220101-shp/communes-20220101.shp")
dfShape <- st_read(pathshp)
dfShape$codecommune <- as.character(dfShape$insee)
dfShape <- dfShape %>%
  # select(geometry, codecommune) %>%
  mutate(codecommune = as.character(codecommune)) %>%
  mutate(codecommune = sub("^0+", "", codecommune))

# Load election data
if (file.exists(paste0(path_data, "election.RData"))) {
  load(paste0(path_data, "election.RData"))
} else {
  message("Data environment does not exist")
}


# Create the combined dataset to determine the common scale
df1988 <- preprocess_data(df_elec, dfShape, "FN1988")
df2002 <- preprocess_data(df_elec, dfShape, "FN2002")

colnames(df1988)[colnames(df1988) == "FN1988"] <- "FN"
colnames(df2002)[colnames(df2002) == "FN2002"] <- "FN"

combined_df <- rbind(df1988 %>% mutate(year = 1988), 
                     df2002 %>% mutate(year = 2002))


# Extract the common scale limits
common_scale_limits <- c(0, 0.5)

# Create map plot function with a common scale
create_map_plot <- function(data, var, title) {
  ggplot(data) +
    geom_sf(aes_string(geometry = "geometry", fill = var), color = NA) +
    scale_fill_gradient(name = "Share of votes", na.value = "white", low = "white", high = "black", limits = common_scale_limits) +
    theme_minimal() +
    labs(title = title) +
    theme(plot.title = element_text(hjust = 0.5))
}

# Update plots with the common scale
p1 <- create_map_plot(df1988, "FN", "")
p2 <- create_map_plot(df2002, "FN", "")

# Combine the plots into one figure
combined_plot <- plot_grid(p1, p2, ncol = 1, labels = c("Panel A: FN vote share in 1988", 
                                                        "Panel B: FN vote share in 2002"))

# Define the file path for saving the combined plot
file_path <- paste0(path_figures, "FN1988_FN2002_combined.png")

# Save the combined plot as a PNG file
ggsave(file_path, plot = combined_plot, device = "png", width = 10, height = 16)

rm(combined_df, combined_plot, df_elec, df1988, df2002, dfShape, p1, p2, common_scale_limits, file_path, pathshp, create_map_plot, preprocess_data)
```

## Figure 5: Nonparametric Effect of several Locality Characteristics, as of 2002 on Support for FN over Time
```{r }
if (file.exists(paste0(path_data, "FN_growth.RData"))) {
  load(paste0(path_data, "FN_growth.RData"))
} else {
  message("Data environment does not exist")
}

# Choose reference year 
ref_year <- 2002
```

```{r }

# Define function to create model and plot for a specified variable
create_model_plot <- function(df, x, ref_year, title) {
  # Create a copy of the dataset and add the variable of interest
  df$x <- df[[x]]
  
  # Create baseline dataframe for reference year
  dfX <- df %>%
    filter(year == ref_year) %>%
    select(codecommune, x) %>%
    distinct(codecommune, .keep_all = TRUE)  # Keep only unique rows for codecommune
  
  # Prepare data for model estimation
  df <- df %>%
    select(-x) %>%
    left_join(dfX, by = "codecommune")
  
  pdata <- df %>%
    select(codecommune, year, FN, reg, x) %>%
    mutate(
      year = factor(as.numeric(year)),
      EU = ifelse(year %in% c("2017", "1994", "1999", "2004", "2014", "2019"), 1, 0)
    )
  
  # Relevel year to set reference
  pdata$year <- relevel(pdata$year, ref = as.character(ref_year))
  
  # Define model formula
  formula <- FN ~ factor(reg):factor(year) + x:factor(year) + factor(EU)
  
  # Estimate model
  model <- lm(formula, data = pdata)
  model_summary <- summary(model)
  
  # Extract coefficients and standard errors for interaction terms
  years <- setdiff(unique(pdata$year), ref_year)
  coefficients_df <- sapply(years, function(y) model_summary$coefficients[paste0("factor(year)", y, ":x"), "Estimate"])
  sd_df <- sapply(years, function(y) model_summary$coefficients[paste0("factor(year)", y, ":x"), "Std. Error"])
  
  # Insert zero for reference year
  position <- which.max(years > ref_year) - 1
  years <- append(years, ref_year, position)
  coefficients_df <- append(coefficients_df, 0, position)
  sd_df <- append(sd_df, 0, position)
  
  # Create dataframe for plotting
  to_plot <- data.frame(
    year = as.numeric(as.character(years)),
    estimate = coefficients_df,
    sd = sd_df
  ) %>% arrange(year)
  
  # Generate plot
  plot <- ggplot(to_plot, aes(x = year, y = estimate)) +
    geom_point(color = "black", size = 0.3) +
    geom_line(color = "black") +
    geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd), width = 1, color = "black") +
    labs(title = title) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
      plot.title = element_text(hjust = 0.5),
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    ) +
    geom_vline(xintercept = ref_year, linetype = "dashed", color = "black") +
    scale_x_continuous(breaks = to_plot$year, labels = to_plot$year)
  
  return(plot)
}

# List of variables and titles to loop through
variables <- list("pop" = "Panel A: population size",
                  "educNoDiplomaPerK" = "Panel B: proportion of no diploma",
                  "ratEmp" = "Panel C: proportion of employed",
                  "pouvr" = "Panel D: proportion of manual workers",
                  "min_distance_to_agglo" = "Panel E: distance to closest agglomeration"
                  )

# Generate plots for each variable
plots <- lapply(names(variables), function(var) {
  create_model_plot(dfZRR1995Controls, var, ref_year, variables[[var]])
})

# Combine and save plots
combined_plot <- grid.arrange(grobs = plots, ncol = 2,
                              left = textGrob("Coefficient Estimates", gp = gpar(fontsize = 15), rot = 90),
                              bottom = textGrob("Years", gp = gpar(fontsize = 15)))

# Export the combined plot
file_path <- paste0(path_figures, "FE_figures.png")
ggsave(file_path, combined_plot, width = 10, height = 8)


rm(list = setdiff(ls(), keep_vars))

```



## Figure 6: FN vs. Pop for different years
```{r }
if (file.exists(paste0(path_data, "FN_growth.RData"))) {
  load(paste0(path_data, "FN_growth.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }


create_plotsFN <- function(dfReg, x_var, var_lab, years, num_bins = 50, min_pop = 70000000) {
  
  # Filter and select relevant data
  dfReg_filtered <- dfReg %>%
    filter(year %in% years) %>%
    select(codecommune, year, x = {{x_var}}, FN, pop, treatment)
  
  dfReg_filtered <- dfReg_filtered %>%
    group_by(codecommune, year) %>%
    filter(n() == 1) %>%
    ungroup()
  
  dfReg_wide <- dfReg_filtered %>%
    pivot_wider(names_from = year, values_from = c(x, FN, pop, treatment), names_sep = "") %>%
    filter_all(all_vars(!is.infinite(.)))
  
  panel_labels <- paste0("Panel ", LETTERS[1:length(years)], ": ")
  plots <- list()
  i <- 1
  
  for (year in years) {
    xVar <- paste0("x", year)
    yVar <- paste0("FN", year)
    popVar <- paste0("pop", year)
    treatVar <- paste0("treatment", year)

    df_plot <- dfReg_wide #%>%
      #filter(!!sym(popVar) < min_pop)

    df_plot[[xVar]] <- as.numeric(df_plot[[xVar]])
    df_plot[[yVar]] <- as.numeric(df_plot[[yVar]])
    
    correlation <- cor(df_plot[[xVar]], df_plot[[yVar]], use = "complete.obs", method = "pearson")
    obs <- nrow(df_plot)
    
    bin_var <- paste0(xVar, "_bins")
    #df_plot <- df_plot %>%
    #  mutate(!!bin_var := cut(!!sym(xVar), breaks = num_bins, labels = FALSE)) %>%
    #  group_by(!!sym(bin_var)) %>%
    #  summarise(
    #    bin_center = mean(!!sym(xVar), na.rm = TRUE),
    #    mean_y = mean(!!sym(yVar), na.rm = TRUE),
    #    count = n(),
    #    .groups = 'drop'
    #  )
    
    
df_plot <- df_plot %>%
  mutate(!!bin_var := ntile(!!sym(xVar), num_bins)) %>%
  group_by(!!sym(bin_var)) %>%
  summarise(
    bin_center = mean(!!sym(xVar), na.rm = TRUE),
    mean_y = mean(!!sym(yVar), na.rm = TRUE),
    count = n(),
    .groups = 'drop'
  )

    
    
    # X-axis breaks and labels: show population sizes, not logs
    breaks <- log(c(100, 1000, 10000, 100000, 1000000))
    labels <- scales::comma(c(0.1, 1, 10, 100, 1000))

# Identify treated units for this year
treated_df <- dfReg_wide %>%
  filter(!is.na(!!sym(xVar))) %>%
  filter(!!sym(treatVar) == TRUE)

# Create the base plot 
p <- ggplot(df_plot, aes(x = bin_center, y = mean_y)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "black") +
  # Add rug or density below the plot for treated units
  # geom_rug(data = treated_df, aes(x = !!sym(xVar)), sides = "b", inherit.aes = FALSE, alpha = 0.2, color = "red") +
  # If you prefer a density line instead of rugs
  geom_density(data = treated_df, aes(x = !!sym(xVar)), inherit.aes = FALSE, 
                color = "red", fill = "red", alpha = 0.1)

  labs(title = paste0(panel_labels[i], year)) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation, 2)),
           hjust = 2, vjust = 1.5, size = 3, color = "black") +
  annotate("text", x = Inf, y = Inf, label = paste0("obs = ", obs),
           hjust = 2.5, vjust = 1.5, size = 3, color = "black") +
  scale_y_continuous(limits = c(0, 0.35)) +
  scale_x_continuous(
    breaks = breaks,
    labels = labels,
    limits = c(0, log(100000))
  ) +
  theme_minimal() +
  theme(axis.title = element_blank())
    
    
    # p <- ggplot(df_plot, aes(x = bin_center, y = mean_y)) +
    #   geom_point(alpha = 0.5) +
    #   geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "black") +
    #   labs(title = paste0(panel_labels[i], year)) +
    #   annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation, 2)),
    #            hjust = 2, vjust = 1.5, size = 3, color = "black") +
    #   annotate("text", x = Inf, y = Inf, label = paste0("obs = ", obs),
    #            hjust = 2.5, vjust = 1.5, size = 3, color = "black") +
    #   scale_y_continuous(limits = c(0, 0.35)) +
    #   scale_x_continuous(
    #     breaks = breaks,
    #     labels = labels,
    #     limits= c(0, log(100000))
    #   ) +
    #   theme_minimal() +
    #   theme(axis.title = element_blank())

    plots[[i]] <- p
    i <- i + 1
  }
  
  n_plots <- length(plots)
  n_cols <- ceiling(sqrt(n_plots))
  n_rows <- ceiling(n_plots / n_cols)
  
  grid.arrange(
    arrangeGrob(grobs = plots, nrow = n_rows, ncol = n_cols),
    bottom = textGrob(var_lab, gp = gpar(fontsize = 14)),
    left = textGrob("FN Vote Share", rot = 90, gp = gpar(fontsize = 14))
  )
}



create_plotsFN <- function(dfReg, x_var, var_lab, years, num_bins = 50, min_pop = 70000000) {
  
  dfReg_filtered <- dfReg %>%
    filter(year %in% years) %>%
    select(codecommune, year, x = {{x_var}}, FN, pop, treatment) %>%
    group_by(codecommune, year) %>%
    filter(n() == 1) %>%
    ungroup()
  
  dfReg_wide <- dfReg_filtered %>%
    pivot_wider(names_from = year, values_from = c(x, FN, pop, treatment), names_sep = "") %>%
    filter_all(all_vars(!is.infinite(.)))
  
  panel_labels <- paste0("Panel ", LETTERS[1:length(years)], ": ")
  plots <- list()
  i <- 1
  
  for (year in years) {
    xVar <- paste0("x", year)
    yVar <- paste0("FN", year)
    popVar <- paste0("pop", year)
    treatVar <- paste0("treatment", year)
    
    df_plot <- dfReg_wide
    
    df_plot[[xVar]] <- as.numeric(df_plot[[xVar]])
    df_plot[[yVar]] <- as.numeric(df_plot[[yVar]])
    
    correlation <- cor(df_plot[[xVar]], df_plot[[yVar]], use = "complete.obs", method = "pearson")
    obs <- nrow(df_plot)
    
    bin_var <- paste0(xVar, "_bins")
    df_plot <- df_plot %>%
      mutate(!!bin_var := cut(!!sym(xVar), breaks = num_bins, labels = FALSE)) %>%
      group_by(!!sym(bin_var)) %>%
      summarise(
        bin_center = mean(!!sym(xVar), na.rm = TRUE),
        mean_y = mean(!!sym(yVar), na.rm = TRUE),
        count = n(),
        .groups = 'drop'
      )
    
    breaks <- log(c(100, 1000, 10000, 100000, 1000000))
    labels <- scales::comma(c(0.1, 1, 10, 100, 1000))
    
    treated_df <- dfReg_wide %>%
      filter(!is.na(!!sym(xVar))) %>%
      filter(!!sym(treatVar) == TRUE)
    
# Scale factor to bring density to the [0, 0.35] range visually
density_scale <- 0.35

# Compute density manually for treated units
density_data <- density(treated_df[[xVar]], na.rm = TRUE)
density_df <- data.frame(x = density_data$x, y = density_data$y * density_scale / max(density_data$y))

p <- ggplot(df_plot, aes(x = bin_center, y = mean_y)) +
  # Add the scaled density as a transparent ribbon in the background
  geom_area(data = density_df, aes(x = x, y = y), 
            inherit.aes = FALSE, fill = "red", alpha = 0.1) +
  geom_line(data = density_df, aes(x = x, y = y), 
            inherit.aes = FALSE, color = "red", alpha = 0.4, linewidth = 0.5) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "black") +
  labs(title = paste0(panel_labels[i], year)) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation, 2)),
           hjust = 2, vjust = 1.5, size = 3, color = "black") +
  annotate("text", x = Inf, y = Inf, label = paste0("obs = ", obs),
           hjust = 2.5, vjust = 1.5, size = 3, color = "black") +
  scale_y_continuous(limits = c(0, 0.35)) +
  scale_x_continuous(
    breaks = breaks,
    labels = labels,
    limits = c(0, log(100000))
  ) +
  theme_minimal() +
  theme(axis.title = element_blank())
    
    plots[[i]] <- p
    i <- i + 1
  }
  
  n_plots <- length(plots)
  n_cols <- ceiling(sqrt(n_plots))
  n_rows <- ceiling(n_plots / n_cols)
  
  grid.arrange(
    arrangeGrob(grobs = plots, nrow = n_rows, ncol = n_cols),
    bottom = textGrob(var_lab, gp = gpar(fontsize = 14)),
    left = textGrob("FN Vote Share", rot = 90, gp = gpar(fontsize = 14))
  )
}

```

```{r }
dfZRR1995Controls[["logpop"]] <- log(dfZRR1995Controls[["pop"]])


combined_plot <- create_plotsFN(dfZRR1995Controls,
               x_var = "logpop",
               var_lab = "Population size (log-scale), in 1,000",
               years = c("1988", "1995", "2002", "2007",  "2017", "2022"),
               num_bins = 50 ,
               min_pop=10000000
               )



# Export the combined plot
file_path <- paste0(path_figures, "pop_vs_FN.png")
ggsave(file_path, combined_plot, width = 10, height = 8)



rm(list = setdiff(ls(), keep_vars))

```









## Figure 7: Comparison of FN vote share between 1988 and 2002 by Treatment Status

```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}

dfZRRControls <- dfZRRControls %>% filter(year_treat > 0)
dfZRRControls <- dfZRRControls %>%
    filter(!is.na(FN) ) %>%
  filter(!is.na(treatment) )

dfZRRControls <- dfZRRControls %>%
  # Calculate the time since treatment
  mutate(time_since_open = year - year_treat) 

# We keep the years 1988 and 2002 when we have elections results
dfSpe <- dfZRRControls  %>%
  filter(year %in% list(1988,2002) )

# create treated var
dfSpe$treated = ifelse( dfSpe$year_treat == 1995, 1, 0)
# Create a dummy variable to indicate the time when the treatment started. Lets assume that treatment started in 1995. In this case, years before 1995 will have a value of 0 and 1995+ a 1. 

dfSpe$post = ifelse(dfSpe$year >= 1995, 1, 0)

controls <- names(dfSpe)
controls <- setdiff(controls, c("codecommune", "nomcommune", "dep", "year", "reg", "nom", "codecommune", "time_since_open", "year_treat", "post", "FN_log", "treated", "FN", "treatment", "FN1995", "RPR", "deltaFN", "turnout_2002", "canton"))

variables_to_clean <- c("codecommune", "FN", "year", controls, "post", "treated",  "year_treat", "time_since_open",  "dep")

# Loop through each variable in the list
for(var in variables_to_clean) {
  # Remove rows with NA values in the current variable
  dfSpe <- dfSpe[!is.na(dfSpe[[var]]), ]
  
  # Remove rows with Inf or -Inf values in the current variable
  dfSpe <- dfSpe[!is.infinite(dfSpe[[var]]), ]
}

dfSpe <- dfSpe %>%
  select(all_of(variables_to_clean))

dfSpe$did <- dfSpe$post * dfSpe$treated

dfSpe <- dfSpe %>%
  distinct()


## Plot 1
df_spread <- dfSpe[dfSpe$year %in% c(1988, 2002), ] %>% 
  select(codecommune, FN, year, year_treat) %>%
  pivot_wider(
    names_from = year,
    values_from = FN,
    values_fill = list(FN = NA),
    values_fn = list(FN = mean)
  ) %>%
  group_by(codecommune) %>%
  mutate(year_treat = max(year_treat, na.rm = TRUE)) %>%
  ungroup() %>%
  rename(FN1988 = `1988`, FN2002 = `2002`) %>%
  drop_na(FN1988, FN2002)

common_aes <- aes(shape = as.factor(year_treat == 1995))

plot1 <- ggplot(df_spread %>% filter(FN1988 < 0.3), aes(x = FN1988, y = FN2002, shape = as.factor(year_treat == 1995), color = as.factor(year_treat == 1995))) +
  stat_summary_bin(
    aes(group = as.factor(year_treat == 1995)),
    fun = "mean",
    bins = 50,  # Adjust the number of bins as needed
    geom = "point", 
    size = 1.5, 
    #alpha = 0.5
  ) +
  scale_shape_manual(
    values = c("TRUE" = 17, "FALSE" = 15),  # Use triangles and squares
    labels = c("after 2004", "1995"),  # Corrected label order
    name = "Year of admission to the ZRR"  # Updated title
  ) +
  scale_color_manual(
    values = c("TRUE" = "black", "FALSE" = "grey"),  # Set colors for TRUE and FALSE points
    labels = c("after 2004", "1995"),  # Corrected label order
    name = "Year of admission to the ZRR"
  ) +
  labs(x = "Vote share for FN in 1988", y = "Vote share for FN in 2002", title = "Panel A: no control") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Position the legend at the bottom

## Plot 2
df_filtered <- dfZRRControls %>%
  filter(year %in% c(1988, 2002)) %>%
  mutate(
    treated = ifelse(year_treat == 1995, 1, 0),
    post = ifelse(year >= 1995, 1, 0)
  ) %>%
  select(codecommune, FN, post, treated, year_treat, time_since_open, dep, all_of(controls)) %>%
  filter(across(everything(), ~ !is.na(.))) %>%
  arrange(codecommune)


for(var in setdiff(variables_to_clean, "year")) {
  # Remove rows with NA values in the current variable
  df_filtered <- df_filtered[!is.na(df_filtered[[var]]), ]
  
  # Remove rows with Inf or -Inf values in the current variable
  df_filtered <- df_filtered[!is.infinite(df_filtered[[var]]), ]
}

# Count occurrences of each codecommune
commune_counts <- df_filtered %>%
  count(codecommune) %>%
  filter(n == 2)

# Filter the original dataframe to keep only those with exactly 2 occurrences
df_filtered <- df_filtered %>%
  filter(codecommune %in% commune_counts$codecommune)

duplicates <- df_filtered %>%
  group_by(codecommune, year_treat, post) %>%
  filter(n() > 1) %>%
  arrange(codecommune, year_treat, post)
rm(duplicates)

df_filtered <- df_filtered %>%
  group_by(codecommune, year_treat, post) %>%
  filter(row_number() == 1) %>%
  ungroup()


# Reshape the data so that each commune has both years' FN values in the same row
df_spread <- df_filtered %>%
  pivot_wider(
    names_from = post, 
    values_from = c(FN, controls),
    values_fill = list(FN = NA)
  ) %>%
  ungroup()


# Create control variables for regression
create_controls <- function(postfix) {
  paste0(controls, "_", postfix)
}

controls_0 <- create_controls("0")
controls_1 <- create_controls("1")


# Create the formulas
formula_0 <- as.formula(paste("FN_0 ~", paste(controls_0, collapse = " + "), "+dep"))
formula_1 <- as.formula(paste("FN_1 ~", paste(controls_0, collapse = " + "), "+dep")) # we regress on pre-treatment controls

df_spread_0 <- df_spread %>% select(c("codecommune", "FN_0", "year_treat", controls_0, "dep"))
df_spread_1 <- df_spread %>% select(c("codecommune", "FN_1", "dep"))

df_spread_0 <- df_spread_0 %>%
  group_by(codecommune) %>%
  fill(FN_0, dep, matches("_0$"), .direction = "downup") %>%
  ungroup() %>%
  distinct()  # Remove duplicate rows



for(var in names(df_spread_0)) {
  # Remove rows with NA values in the current variable
  df_spread_0 <- df_spread_0[!is.na(df_spread_0[[var]]), ]
    df_spread_0 <- df_spread_0[!is.infinite(df_spread_0[[var]]), ]
}
for(var in names(df_spread_1)) {
  # Remove rows with NA values in the current variable
  df_spread_1 <- df_spread_1[!is.na(df_spread_1[[var]]), ]
    df_spread_1 <- df_spread_1[!is.infinite(df_spread_1[[var]]), ]
}



df_spread <- inner_join(df_spread_0, df_spread_1, by = c("codecommune",  "dep"))


# Run the regressions
regression_0 <- lm(formula_0, data = df_spread)
regression_1 <- lm(formula_1, data = df_spread)


# Extract residuals from each regression
residuals_0 <- residuals(regression_0)
residuals_1 <- residuals(regression_1)


# Combine residuals into a data frame
residuals_df <- data.frame(
  year_treat = df_spread$year_treat,
  residuals_0 = residuals_0,
  residuals_1 = residuals_1
)



plot2 <- ggplot(residuals_df, aes(x = residuals_0, y = residuals_1, shape = as.factor(year_treat == 1995), color = as.factor(year_treat == 1995))) +
  stat_summary_bin(
    aes(group = as.factor(year_treat == 1995)),
    fun = "mean",
    bins = 50,  # Adjust the number of bins as needed
    geom = "point", 
    size = 1, 
    #alpha = 0.5
  ) +
  scale_shape_manual(
    values = c("TRUE" = 17, "FALSE" = 15),  # Use triangles and squares
    labels = c("1995", "after 2004"),  # Updated labels
    name = "Year of admission to the ZRR"  # Updated title
  ) +
  scale_color_manual(
    values = c("TRUE" = "black", "FALSE" = "grey"),  # Set colors for TRUE and FALSE points
    labels = c("1995", "after 2004"),
    name = "Year of admission to the ZRR"
  ) +
  labs(x = "Vote share for FN in 1988 (residuals)", y = "Vote share for FN in 2002 (residuals)", title = "Panel B: with controls") +
  theme_minimal() +
  theme(legend.position = "none")  # Hide legend



## Combine plot1 and plot2

combined_plot <- plot1 / plot2 

# Define the path where the combined plot will be saved
path <- paste0(path_figures, "combined_plot_FN1988_FN2002.png")

# Save the combined plot as a PNG file
ggsave(filename = path, plot = combined_plot, width = 12, height = 12, dpi = 300)


rm(list = setdiff(ls(), keep_vars))

```

## Figure 7-bis: Comparison of FN vote share between 1988 and 2002 by Treatment Status, all population vs. 1995

```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}

dfZRRControls <- dfZRRControls %>%
    filter(!is.na(FN) ) %>%
  filter(!is.na(treatment) )

# We keep the years 1988 and 2002 when we have elections results
dfSpe <- dfZRRControls  %>%
  filter(year %in% list(1988,2002) )

# create treated var
dfSpe$treated = ifelse( dfSpe$year_treat == 1995, 1, 0)
# Create a dummy variable to indicate the time when the treatment started. Lets assume that treatment started in 1995. In this case, years before 1995 will have a value of 0 and 1995+ a 1. 

dfSpe$post = ifelse(dfSpe$year >= 1995, 1, 0)

controls <- names(dfSpe)
controls <- setdiff(controls, c("codecommune", "nomcommune", "dep", "year", "reg", "nom", "codecommune",  "post", "FN_log", "treated", "FN", "treatment", "FN1995", "RPR", "deltaFN", "turnout_2002", "canton"))

variables_to_clean <- c("codecommune", "FN", "year", controls, "post", "treated",   "dep")

# Loop through each variable in the list
for(var in variables_to_clean) {
  # Remove rows with NA values in the current variable
  dfSpe <- dfSpe[!is.na(dfSpe[[var]]), ]
  
  # Remove rows with Inf or -Inf values in the current variable
  dfSpe <- dfSpe[!is.infinite(dfSpe[[var]]), ]
}

dfSpe <- dfSpe %>%
  select(all_of(variables_to_clean))

dfSpe$did <- dfSpe$post * dfSpe$treated

dfSpe <- dfSpe %>%
  distinct()


## Plot 1
df_spread <- dfSpe[dfSpe$year %in% c(1988, 2002), ] %>% 
  select(codecommune, FN, year, treated) %>%
  pivot_wider(
    names_from = year,
    values_from = FN,
    values_fill = list(FN = NA),
    values_fn = list(FN = mean)
  ) %>%
  rename(FN1988 = `1988`, FN2002 = `2002`) %>%
  drop_na(FN1988, FN2002)

common_aes <- aes(shape = as.factor(treated == TRUE))

plot1 <- ggplot(df_spread %>% filter(FN1988 < 0.3), aes(x = FN1988, y = FN2002, shape = as.factor(treated == TRUE), color = as.factor(treated == TRUE))) +
  stat_summary_bin(
    aes(group = as.factor(treated == TRUE)),
    fun = "mean",
    bins = 50,  # Adjust the number of bins as needed
    geom = "point", 
    size = 1.5, 
    #alpha = 0.5
  ) +
  scale_shape_manual(
    values = c("TRUE" = 17, "FALSE" = 15),  # Use triangles and squares
    labels = c("Never treated or treated after 2004", "Treated in 1995"),  # Corrected label order
    name = "Treatment Status"  
  ) +
  scale_color_manual(
    values = c("TRUE" = "black", "FALSE" = "grey"),  # Set colors for TRUE and FALSE points
    labels = c("Never treated or treated after 2004", "Treated in 1995"),  # Corrected label order
    name = "Treatment Status"  
  ) +
  labs(x = "Vote share for FN in 1988", y = "Vote share for FN in 2002", title = "Panel A: no control") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Position the legend at the bottom

## Plot 2
df_filtered <- dfZRRControls %>%
  filter(year %in% c(1988, 2002)) %>%
  mutate(
    treated = ifelse(year_treat == 1995, 1, 0),
    post = ifelse(year >= 1995, 1, 0)
  ) %>%
  select(codecommune, FN, post, treated, dep, all_of(controls)) %>%
  filter(across(everything(), ~ !is.na(.))) %>%
  arrange(codecommune)


# Create 2 dataframes: one with the data when post=0 (FN and controls), one with FN and codecommune when post=1
df_0 <- df_filtered %>%
  filter(post==0) %>%
  group_by(codecommune) %>%
  filter(row_number() == 1) %>%
  ungroup()


df_1 <- df_filtered %>%
  filter(post==1) %>%
  select(FN, codecommune) %>%
  rename(FN_post = FN) %>%
  group_by(codecommune) %>%
  filter(row_number() == 1) %>%
  ungroup()

df_spread <- inner_join(df_0, df_1, by = c("codecommune"))


for(var in setdiff(variables_to_clean, "year")) {
  # Remove rows with NA values in the current variable
  df_spread <- df_spread[!is.na(df_spread[[var]]), ]
  
  # Remove rows with Inf or -Inf values in the current variable
  df_spread <- df_spread[!is.infinite(df_spread[[var]]), ]
}


# Create the formulas
formula_0 <- as.formula(paste("FN ~", paste(setdiff(controls, "year_treat"), collapse = " + "), "+dep"))
formula_1 <- as.formula(paste("FN_post ~", paste(setdiff(controls, "year_treat"), collapse = " + "), "+dep")) # we regress on pre-treatment controls


# Run the regressions
regression_0 <- lm(formula_0, data = df_spread)
regression_1 <- lm(formula_1, data = df_spread)


# Extract residuals from each regression
residuals_0 <- residuals(regression_0)
residuals_1 <- residuals(regression_1)


# Combine residuals into a data frame
residuals_df <- data.frame(
  treated = df_spread$treated,
  residuals_0 = residuals_0,
  residuals_1 = residuals_1
)



plot2 <- ggplot(residuals_df, aes(x = residuals_0, y = residuals_1, shape = as.factor(treated == TRUE), color = as.factor(treated == TRUE))) +
  stat_summary_bin(
    aes(group = as.factor(treated == TRUE)),
    fun = "mean",
    bins = 50,  # Adjust the number of bins as needed
    geom = "point", 
    size = 1, 
    #alpha = 0.5
  ) +
  scale_shape_manual(
    values = c("TRUE" = 17, "FALSE" = 15),  # Use triangles and squares
    labels = c("Never treated or treated after 2004", "Treated in 1995"),  # Corrected label order
    name = "Treatment Status"  
  ) +
  scale_color_manual(
    values = c("TRUE" = "black", "FALSE" = "grey"),  # Set colors for TRUE and FALSE points
    labels = c("Never treated or treated after 2004", "Treated in 1995"),  # Corrected label order
    name = "Treatment Status"  
  ) +
  labs(x = "Vote share for FN in 1988 (residuals)", y = "Vote share for FN in 2002 (residuals)", title = "Panel B: with controls") +
  theme_minimal() +
  theme(legend.position = "none")  



## Combine plot1 and plot2

combined_plot <- plot1 / plot2 

# Define the path where the combined plot will be saved
path <- paste0(path_figures, "combined_plot_FN1988_FN2002_all_pop.png")

# Save the combined plot as a PNG file
ggsave(filename = path, plot = combined_plot, width = 12, height = 12, dpi = 300)


rm(list = setdiff(ls(), keep_vars))

```



## Figure 8: Map and distribution of the running variable
```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}


# Set the file path
pathshp <- "/Users/ilanpargamin/Desktop/thesis/THESIS_REPRO/DATA/communes-20220101-shp/communes-20220101.shp"
# Load the shape file into a dataframe
dfShape <- st_read(pathshp)
dfShape$codecommune <- as.character(dfShape$insee)
dfShape <- dfShape %>%
  select(geometry, codecommune)

dfShape <- dfShape %>%
  mutate(codecommune = as.character(codecommune)) %>%
  mutate(codecommune = sub("^0+", "", codecommune))


dfDistance <- dfDistance %>%
  mutate(codecommune = as.character(codecommune)) %>%
  mutate(codecommune = sub("^0+", "", codecommune)) 

dfMap <- dfDistance  %>%
  select(codecommune, treatment, distance_to_border)

dfMap <- merge(dfMap, dfShape, on="codecommune")

dfMap <- st_as_sf(dfMap)


# Filter areas where treatment = 1 and aggregate
treatment_areas <- dfMap %>%
  filter(treatment == 1)

# Aggregate all touching polygons into one using st_union
treatment_union <- st_union(treatment_areas$geometry)

# Create a new sf object for the union for easier plotting
treatment_union_sf <- st_sf(geometry = st_sfc(treatment_union))

# Update var if necessary
var <- "x"

# Categorize 'distance_to_border'
dfMap$distance_to_border[is.na(dfMap$distance_to_border)] <- 65000
dfMap$x <- cut(dfMap$distance_to_border, breaks = c(-60000, -5000, 5000, 60000, 200000),
               labels = c( "-60 - -5", "-5 - 5", "5-60", "above 60"), include.lowest = TRUE)


# Plot the original map with highlighted treatment areas
map <- ggplot(dfMap) +
  geom_sf(aes(fill = as.factor(!!sym(var)), geometry = geometry), color = NA) +  # Fill color by distance_to_border categories
  scale_fill_manual(values = c("black", "gray30", "gray60", "grey90")) +  # Specify grayscale colors for each category
  geom_sf(data = treatment_union_sf, fill = NA, color = "black", size = 0.01, lwd = 0.05) +
  theme_minimal() +
  theme(legend.position = "bottom") +  # Move legend to the bottom
  guides(fill = guide_legend(title = "Distance Category (km)")) 


```

```{r }
bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
# df_rdd$deltaFN2002_1988 <- df_rdd$y # voteShareFN2002 or deltaFN2002_1988 or voteShareFN1988
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)

df_rdd <- df_rdd[!duplicated(df_rdd$codecommune), ]





p_dis <- ggplot(df_rdd, 
       aes(x = dist, fill = treatmentZRR)) +
  geom_histogram(color = "white", boundary = 0, bins=100) +
  scale_fill_manual(values = c("gray80", "gray20"), labels = c("Below 0: In program", "Above 0: Not in program")) +
  labs(x = "Distance to program frontier (meter)", y = "Number of localities") +
  theme_minimal() +
  theme(legend.position = c(0.8, 0.8),  # Adjust the position inside the graph
        legend.title = element_blank(),  # Remove legend title
        legend.background = element_rect(fill = "white", color = "black"),  # Optional: add a border around the legend
        legend.box.background = element_rect(color = "black"))  # Optional: add a border around the legend box



combined_plot <- map / p_dis + plot_layout(ncol = 1)

file_path <- paste0(path_figures, "distanceMapAndDistribution_test.png")

# Save the plot as a PNG file
ggsave(file_path, plot = combined_plot, device = "png", width = 10, height = 12)

rm(combined_plot, df_rdd, dfDistance, dfMap, dfShape, dfZRR1995Controls, map, p_dis, treatment_areas, treatment_union_sf, treatment_union, controls, file_path, pathshp, pattern, threshold, upper_bound, var, y)
```


## Figure 9: Vote Share for FN in 1988 (placebo test)
```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
# df_rdd$deltaFN2002_1988 <- df_rdd$y # voteShareFN2002 or deltaFN2002_1988 or voteShareFN1988
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)

df_rdd <- df_rdd[!duplicated(df_rdd$codecommune), ]

```

```{r }

y <- "FN1988"
b <- 10000
  
df_filtered <- filter(df_rdd, x >= -b & x <= b)
  
# Fit regression model and extract residuals
conditional_on <- c("z", "x", controls, "factor(dep)")

formula <- as.formula(paste(y, "~", paste(conditional_on, collapse = " + ")))
model <- lm(formula, data = df_filtered)
df_filtered$residuals <- residuals(model)
    
# Compute clustered standard errors at the "canton" level
clustered_se <- coef_test(model, cluster = df_filtered$canton, vcov = "CR2")

# Extract coefficient, clustered standard error, and p-value for `zTRUE`
z_coeff <- summary(model)$coefficients["zTRUE", "Estimate"]
z_se <- clustered_se["zTRUE", "SE"]
z_pvalue <- clustered_se["zTRUE", "p_Satt"]

# Determine significance stars based on p-value
significance <- ifelse(z_pvalue < 0.001, "***", 
                       ifelse(z_pvalue < 0.01, "**", 
                              ifelse(z_pvalue < 0.05, "*", "")))

# Format the text for annotation
annotation_text <- paste0(
  "Coef. Treatment: ",
  format(round(z_coeff, 4), nsmall = 4, scientific = FALSE),
  " (",
  format(round(z_se, 4), nsmall = 4, scientific = FALSE),
  ") ",
  significance
)

# Create plot with residuals
p <- ggplot(df_filtered, aes(x = x, 
                             y = residuals)) +
  stat_summary_bin(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                   fun = "mean",  # Calculates the mean for each bin
                   bins = 15,     # Adjust the number of bins as needed
                   geom = "point", size = 2, color = "black") +
  geom_smooth(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
              method = "lm", formula = y ~ x, size = 1, color = "black") +
  geom_vline(xintercept = 0, color = "black") +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 30, hjust = 0.5),
        axis.title.x = element_blank(),  # Remove x-axis title
        axis.title.y = element_blank()) + # Remove y-axis title
  annotate("text", x = Inf, y = Inf, label = annotation_text, 
           hjust = 1.1, vjust = 1.5, size = 6, color = "black", parse = FALSE)

# Combine plots into one figure with unique x and y labels
file_path <- paste0(path_figures, "RDD_plots_conditional_FN1988.png")

png(file_path, width = 800, height = 900)

combined_plot <- grid.arrange(p, ncol = 1, 
                              bottom = textGrob("Distance to Program Frontier", gp = gpar(fontsize = 20)),
                              left = textGrob("Residuals", rot = 90, gp = gpar(fontsize = 20)))

dev.off()

rm(combined_plot, df_rdd, dfDistance, dfZRR1995Controls, controls, file_path, pattern, threshold, upper_bound, y, model, p, df_filtered, b, formula, conditional_on)





# Bin and compute means + standard errors
binned <- df_filtered %>%
  mutate(bin = cut(x, breaks = 15)) %>%  # 15 bins
  group_by(bin) %>%
  summarise(
    x_center = mean(x, na.rm = TRUE),
    y_mean = mean(residuals, na.rm = TRUE),
    se = sd(residuals, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Plot with error bars
ggplot(binned, aes(x = x_center, y = y_mean)) +
  geom_point(color = "black", size = 2) +
  geom_errorbar(aes(ymin = y_mean - se, ymax = y_mean + se), width = 500, color = "black") +
  geom_vline(xintercept = 0, color = "black") +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 30, hjust = 0.5),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  annotate(
    "text",
    x = Inf, y = Inf,
    label = annotation_text,
    hjust = 1.1, vjust = 1.5,
    size = 6,
    color = "black",
    parse = FALSE
  )


```


## Figure 10: RDD: Balancing Checks

```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)

df_rdd <- df_rdd[!duplicated(df_rdd$codecommune), ]

```

```{r}
labels <- c("FN1988"= "Vote share for FN in 1988",
            "pchom" = "Unemployed (%)", 
            "pop" = "Population", 
            "ratEmp" = "In the labor force (%)", 
            "ratForeigners" = "Foreigners (%)", 
            "asso" = "OPI per 1000 inhabitants", 
            "educNoDiplomaPerK" = "No diploma (%)", 
            "educSUPPerK" = "Academic (%)", 
            "educBACPerK" = "Highschool (%)", 
            "educCAPBEPPerK" = "Technical (%)", 
            "poph" = "Ages 20-40, men", 
            "popf" = "Ages 20-40, women", 
            "pagri" = "Agriculture (%)", 
            "pindp" = "Independant (%)", 
            "ppint" = "Intermediate occupations (%)", 
            "pempl" = "Clerical (%)", 
            "pouvr" = "Manual (%)", 
            "altitude" = "Altitude", 
            "superficie" = "Area", 
            "logVac" = "Vacant housing (%)", 
            "haie" = "Fences per squared km", 
            "vigne" = "Vines per squared km", 
            "revenuPerK" = "Taxable income per capita",
            "delta_pop_1982_1990" = "Population change in p.p. 1982-1990", 
            "delta_emp_1982_1990" = "Change in p.p. in the labor force 1982-1990", 
            "min_distance_to_agglo" = "Distance to closest agglomeration (meters)")

# Split controls into two parts
controls_part1 <- controls[1:ceiling(length(controls)/2)]
controls_part2 <- controls[(ceiling(length(controls)/2) + 1):length(controls)]
controls_part1 <- setdiff(controls_part1, c("border", "pop", "FN1988"))
controls_part2 <- setdiff(controls_part2, c("typologie", "pagri", "superficie"))

conditional_on <- c("z", "x", controls, "factor(dep)")


# Normalize the variables
df_rdd_nor <- df_rdd %>%
  mutate(across(all_of(controls_part1), ~ scale(.) %>% as.vector)) %>%
  mutate(across(all_of(controls_part2), ~ scale(.) %>% as.vector))

create_plots_cond <- function(controls_subset, file_name, b) {
  plots <- list()
  df_filtered <- filter(df_rdd_nor, x >= -b & x <= b)
  
  for (y in controls_subset) {
    # Fit regression model and extract residuals
    formula <- as.formula(paste(y, "~", paste(setdiff(conditional_on, y), collapse = " + ")))
    model <- lm(formula, data = df_filtered)
    df_filtered$residuals <- residuals(model)
    
    # Compute clustered standard errors at the "canton" level
    clustered_se <- coef_test(model, cluster = df_filtered$canton, vcov = "CR2")
    
    # Extract coefficient, clustered standard error, and p-value for `zTRUE`
    z_coeff <- summary(model)$coefficients["zTRUE", "Estimate"]
    z_se <- clustered_se["zTRUE", "SE"]
    z_pvalue <- clustered_se["zTRUE", "p_Satt"]
  
    # Determine significance stars based on p-value
    significance <- ifelse(z_pvalue < 0.001, "***", 
                           ifelse(z_pvalue < 0.01, "**", 
                                  ifelse(z_pvalue < 0.05, "*", "")))
    
    # Format the text for annotation
    annotation_text <- paste0("Coef. Treatment: ", round(z_coeff, 3), " (", round(z_se, 3), ") ", significance)
    
    
    # Create plot with residuals
    p <- ggplot(df_filtered, aes(x = x, y = residuals)) +
      stat_summary_bin(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                       fun = "mean",  # Calculates the mean for each bin
                       bins = 15,     # Adjust the number of bins as needed
                       geom = "point", size = 2, color = "black") +
      geom_smooth(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                  method = "lm", formula = y ~ poly(x, 1), size = 1, color = "black") +
      geom_vline(xintercept = 0, color = "black") +
      labs(title = labels[y]) +  # Use the label as the title
      theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(size = 50, hjust = 0.5),
          axis.title.x = element_blank(),  # Remove x-axis title
          axis.title.y = element_blank()) +  # Remove y-axis title
    annotate("text", x = Inf, y = Inf, label = annotation_text, hjust = 1.1, vjust = 1.5, size = 6, color = "black", parse = FALSE)
  
    plots[[y]] <- p
  }
  
  # Combine plots into one figure
  png(file_name, width = 800 * 3, height = 900 * ceiling(length(controls_subset) / 3))
  
  combined_plot <- grid.arrange(grobs = plots, ncol = 3, 
                                bottom = textGrob("Distance to Program Frontier", gp = gpar(fontsize = 50)),
                                left = textGrob("Residuals", rot = 90, gp = gpar(fontsize = 50)))
  
  dev.off()
}


# Create and save the plots for each part
create_plots_cond(controls_part1, paste0(path_figures, "RDD_plots_conditional_part1.png"), 10000)
create_plots_cond(controls_part2, paste0(path_figures, "RDD_plots_conditional_part2.png"), 10000)



rm(list = setdiff(ls(), keep_vars))

```





## Figure 11: RDD: main outcomes
```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
# df_rdd$deltaFN2002_1988 <- df_rdd$y # voteShareFN2002 or deltaFN2002_1988 or voteShareFN1988
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)

df_rdd <- df_rdd[!duplicated(df_rdd$codecommune), ]

```

```{r }
df_rdd$FN2002 <- df_rdd$y

# Define the labels
labels <- c("FN2002" = "FN vote share in 2002",
            "RPR2002" = "RPR vote share in 2002",
            "turnout_2002" = "Turnout in 2002",
            "FN2007" = "FN vote share in 2007",
            "FN2012" = "FN vote share in 2012")

conditional_on <- c(controls, "dep")

create_plots_cond <- function(controls_subset, file_name, b) {
  plots <- list()
  df_filtered <- filter(df_rdd, x >= -b & x <= b)
  
  for (y in controls_subset) {
    # Fit regression model and extract residuals
    formula <- as.formula(paste(y, "~", paste(conditional_on, collapse = " + ")))
    model <- lm(formula, data = df_filtered)
    df_filtered$residuals <- residuals(model)
    
    # Create plot with residuals in black and white
    p <- ggplot(df_filtered, aes(x = x, y = residuals)) +
      stat_summary_bin(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                       fun = "mean",  # Calculates the mean for each bin
                       bins = 15,     # Adjust the number of bins as needed
                       geom = "point", size = 1.5, color = "black") +
      geom_smooth(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                  method = "lm", formula = y ~ poly(x, 1), size = 1, color = "black") +
      geom_vline(xintercept = 0, color = "black") +
      labs(title = labels[y]) +  # Use the label as the title
      theme_minimal() +
      theme(legend.position = "none",
            plot.title = element_text(size = 50, hjust = 0.5),
            axis.title.x = element_blank(),  # Remove x-axis title
            axis.title.y = element_blank())  # Remove y-axis title
    
    plots[[y]] <- p
  }
  
  # Combine plots into one figure with unique x and y labels
  png(file_name, width = 800 * 3, height = 900 * ceiling(length(controls_subset) / 3))
  
  combined_plot <- grid.arrange(grobs = plots, ncol = 3, 
                                bottom = textGrob("Distance to Program Frontier", gp = gpar(fontsize = 20)),
                                left = textGrob("Residuals", rot = 90, gp = gpar(fontsize = 20)))
  
  dev.off()
}

# Example usage
create_plots_cond(c("FN2002", "RPR2002", "turnout_2002", "FN2007", "FN2012"), paste0(path_figures, "RDD_plots_conditional_outcomes 20000 .png"), 10000)
create_plots_cond(c("FN2002", "RPR2002", "turnout_2002", "FN2007", "FN2012"), paste0(path_figures, "RDD_plots_conditional_outcomes 10000 .png"), 20000)

rm(df_rdd, dfDistance, dfZRR1995Controls, controls, pattern, threshold, upper_bound, y, conditional_on, labels, create_plots_cond)

```

## Figure 12: ZRR effect on later elections
```{r }
# Create the data
df <- data.frame(
  Year = c(2007, 2012, 2017, 2022),
  Bandwidth20000_Estimate = c(-0.004, -0.006, -0.008, -0.007),
  Bandwidth20000_SE = c(0.001, 0.001, 0.002, 0.002),
  Bandwidth10000_Estimate = c(-0.002, -0.003, -0.005, -0.004),
  Bandwidth10000_SE = c(0.001, 0.002, 0.002, 0.002),
  Bandwidth5000_Estimate = c(0.0, -0.002, 0.0, -0.001),
  Bandwidth5000_SE = c(0.002, 0.003, 0.003, 0.003)
)

# Reshape to long format
df_long <- df %>%
  pivot_longer(cols = -Year, names_to = c("Bandwidth", ".value"), names_pattern = "Bandwidth(\\d+)_(.*)") %>%
  mutate(Bandwidth = paste("Bandwidth", Bandwidth))

# Plot
 p <- ggplot(df_long, aes(x = Year, y = Estimate, group = Bandwidth, color = Bandwidth)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = Estimate - SE, ymax = Estimate + SE), width = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "Election Year",
    y = "Coefficient Estimate",
    color = "Bandwidth"
  ) +
  theme_minimal()


# Export the combined plot
file_path <- paste0(path_figures, "later_elections.png")
ggsave(file_path, p, width = 10, height = 8)

rm(list=ls())
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")
keep_vars<- c(path_data, path_out, path_figures, path_tables)

```




## Figure 13: Local Linear Regressions with varying bandwidth: FN Share of vote in 2002

```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
# df_rdd$deltaFN2002_1988 <- df_rdd$y # voteShareFN2002 or deltaFN2002_1988 or voteShareFN1988
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)

df_rdd <- df_rdd[!duplicated(df_rdd$codecommune), ]


# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

```

```{r }

formula <- as.formula(paste("y ~ z + x + ", paste(controls, collapse = " + "), "+ factor(dep)"))

# Define a vector of bandwidths
bandwidths <- seq(1000, 40000, by = 1000) 

# Initialize vectors to store results
coef_zTRUE <- numeric(length(bandwidths))
se_zTRUE <- numeric(length(bandwidths))

# Loop over bandwidths
for (i in seq_along(bandwidths)) {
  bw <- bandwidths[i]
  
  # Filter the data based on the current bandwidth
  df_filtered <- filter(df_rdd, x >= -bw & x <= bw)
  
  # Estimate the model
  model <- lm(formula, data = df_filtered)
  
  # Calculate cluster-robust standard errors
  cluster_se <- coeftest(model, vcov = vcovHC(model, type = "HC1", cluster = "group", cluster.id = df_filtered$canton))
  
  # Store the coefficient of zTRUE and its standard error
  coef_zTRUE[i] <- cluster_se["zTRUE", "Estimate"]
  se_zTRUE[i] <- cluster_se["zTRUE", "Std. Error"]
}

# Create a data frame for plotting
results <- data.frame(
  Bandwidth = bandwidths,
  Coefficient = coef_zTRUE,
  SE = se_zTRUE
)

# Plot the results

plot <- ggplot(results, aes(x = Bandwidth, y = Coefficient)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin = Coefficient - 1.96 * SE, ymax = Coefficient + 1.96 * SE), width = 0.05, alpha=0.5) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 1), se = FALSE, color = "red", linetype = "dashed") +
  labs(#title = "Effect of Bandwidth on Estimated Treatment Effect with Quadratic Fit",
       x = "Bandwidth",
       y = "Estimated Coefficient of LATE") +
  theme_minimal()

ggsave(paste0(path_figures, "bandwidth_effect_plot.png"), plot = plot, width = 8, height = 6)


rm(df_rdd, dfDistance, dfZRR1995Controls, controls, pattern, threshold, upper_bound, y, df_filtered, model, plot, results, bandwidths, bw, cluster_se, coef_zTRUE, formula, i)

```


## Figure 14: Map and Distribution of the running variable - Placebo
```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}



dfMap <- read_excel(paste0(path_data, "dataGeoRDD_canton_random/dataGeoRDD_canton_random1.xlsx")) %>%
  select(-year) %>%
  mutate(
    codecommune = sub("^0+", "", as.character(codecommune)),
    x = distance_to_border,
    z = ifelse(x < 0, 1, 0),
    treatment = z,
    distance_to_border = x
  ) %>%
  select(codecommune, treatment, distance_to_border)


# Load the shape file into a dataframe
dfShape <- st_read("/Users/ilanpargamin/Desktop/thesis/THESIS_REPRO/DATA/communes-20220101-shp/communes-20220101.shp") %>%
  select(geometry, insee) %>%
  mutate(codecommune = sub("^0+", "", as.character(insee))) %>%
  select(-insee)


dfMap <- merge(dfMap, dfShape, on="codecommune")

dfMap <- st_as_sf(dfMap)


# Filter areas where treatment = 1 and aggregate
treatment_areas <- dfMap %>%
  filter(treatment == 1)

# Aggregate all touching polygons into one using st_union
treatment_union <- st_union(treatment_areas$geometry)

# Create a new sf object for the union for easier plotting
treatment_union_sf <- st_sf(geometry = st_sfc(treatment_union))

var <- "x"

# Categorize 'distance_to_border'
dfMap$distance_to_border[is.na(dfMap$distance_to_border)] <- 65000
dfMap$x <- cut(dfMap$distance_to_border, breaks = c(-60000, -5000, 5000, 60000, 200000),
               labels = c( "-60 - -5", "-5 - 5", "5-60", "above 60"), include.lowest = TRUE)


# Plot the original map with highlighted treatment areas
map <- ggplot(dfMap) +
  geom_sf(aes(fill = as.factor(!!sym(var)), geometry = geometry), color = NA) +  # Fill color by distance_to_border categories
  scale_fill_manual(values = c("black", "gray30", "gray60", "gray90")) +  # Specify grayscale colors for each category
  geom_sf(data = treatment_union_sf, fill = NA, color = "black", size = 0.01, lwd = 0.05) +
  theme_minimal() +
  theme(legend.position = "bottom") +  # Move legend to the bottom
  guides(fill = guide_legend(title = "Distance Category (km)")) 


```

```{r }
bandwidth <- 20000 # it corresponds to the distance (in meters)

df_rdd <- subset(dfMap, distance_to_border >= -bandwidth & distance_to_border <= bandwidth) %>%
  mutate(treatmentZRR = ifelse(distance_to_border <= 0, 1, 0))


p_dis <- ggplot(df_rdd, 
       aes(x = distance_to_border, fill = factor(treatmentZRR))) +
  geom_histogram(color = "white", boundary = 0, bins=100) +
  scale_fill_manual(values = c("gray80", "gray20"), labels = c("Below 0: In program", "Above 0: Not in program")) +
  labs(x = "Distance to program frontier (meter)", y = "Number of localities") +
  theme_minimal() +
  theme(legend.position = c(0.8, 0.8),  # Adjust the position inside the graph
        legend.title = element_blank(),  # Remove legend title
        legend.background = element_rect(fill = "white", color = "black"),  # Optional: add a border around the legend
        legend.box.background = element_rect(color = "black"))  # Optional: add a border around the legend box

combined_plot <- map / p_dis + plot_layout(ncol = 1)
file_path <- paste0(path_figures, "distanceMapAndDistribution_placebo.png")

# Save the plot as a PNG file
ggsave(file_path, plot = combined_plot, device = "png", width = 10, height = 12)
```

```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Figure 15: Comparison of Heterogeneity Effects

```{r}

if (file.exists(paste0(processed_data_path, "borders_pair.RData"))) {
  load(paste0(processed_data_path, "borders_pair.RData"))
} else {
  message("Data environment does not exist")
}


controls <- setdiff(controls, c("typologie", "x"))

df_rct <- dfZRRControls %>%
  select(-x, -typologie) %>%
  select(all_of(c("y", "z", "canton", "dep",  "border_pair", controls))) %>%
  rename(treatmentZRR=z)  

my_data <- df_rct[complete.cases(df_rct), ]
my_data$pop <- log(my_data$pop)
my_data$treatmentZRR <- as.numeric(my_data$treatmentZRR)
my_data[] <- lapply(names(my_data), function(x) {
  if (x != "border_pair") {
    as.numeric(my_data[[x]])
  } else {
    my_data[[x]]
  }
})

df <- my_data
rm(my_data, df_rct)




set.seed(142)


# Split data into 3 samples
folds = createFolds(1:nrow(df), k=2)

Y1 <- df[folds[[1]],1]
Y2 <- df[folds[[2]],1]

X1 <- df[folds[[1]],2]
X2 <- df[folds[[2]],2]

W1 <- df[folds[[1]], controls]
W2 <- df[folds[[2]], controls]

### Creates a vector of 0s and a vector of 1s of length n (hack for later usage)
zeros <- function(n) {
  return(integer(n))
}
ones <- function(n) {
  return(integer(n)+1)
}

```


```{r}
# Cate OLS
sl_lm = SuperLearner(Y = Y1, 
                     X = data.frame(X=X1, W1, W1*X1), 
                     family = binomial(), 
                     SL.library = "SL.lm", 
                     cvControl = list(V=0))
ols_pred_0s <- predict(sl_lm, data.frame(X=zeros(nrow(W2)), W2, W2*zeros(nrow(W2))), onlySL = T)
ols_pred_1s <- predict(sl_lm, data.frame(X=ones(nrow(W2)), W2, W2*ones(nrow(W2))), onlySL = T)

cate_ols <- ols_pred_1s$pred - ols_pred_0s$pred


# Post-selection Lasso
lasso = create.Learner("SL.glmnet", params = list(alpha = 1), name_prefix="lasso")

get_lasso_coeffs <- function(sl_lasso) {
  return(coef(sl_lasso$fitLibrary$lasso_1_All$object, s="lambda.min")[-1,])
}  

SL.library <- lasso$names

predict_y_lasso <- SuperLearner(Y = Y1,
                         X = data.frame(treatmentZRR=X1, 
                                        W1, X1*W1), 
                         family = gaussian(),
                         SL.library = SL.library, 
                         cvControl = list(V=0))

kept_variables <- which(get_lasso_coeffs(predict_y_lasso)!=0)

predict_x_lasso <- SuperLearner(Y = X1,
                          X = W1, 
                          family = gaussian(),
                          SL.library = lasso$names, 
                          cvControl = list(V=0))

kept_variables2 <- which(get_lasso_coeffs(predict_x_lasso)!=0) + 1 #+1 to include X

sl_post_lasso <- SuperLearner(Y = Y1,
                              X = data.frame(treatmentZRR=X1, 
                                        W1, X1*W1)[, c(kept_variables, kept_variables2)], 
                                   family = gaussian(),
                                   SL.library = "SL.lm", 
                                   cvControl = list(V=0))


postlasso_pred_0s <- predict(
  sl_post_lasso, 
  data.frame(
    X=zeros(nrow(W2)), 
    W2, 
    W2*zeros(nrow(W2))
                                        )[, c(kept_variables, kept_variables2)], onlySL = T)

postlasso_pred_0s <- predict(
  sl_post_lasso,
  newdata = data.frame(treatmentZRR = rep(0, nrow(W2)), W2, W2 * 0)[, c(kept_variables, kept_variables2)],
  onlySL = TRUE
)


postlasso_pred_1s <- predict(
  sl_post_lasso,
  newdata = data.frame(
    treatmentZRR = rep(1, nrow(W2)),
    W2,
    W2 * 1
  )[, c(kept_variables, kept_variables2)],
  onlySL = TRUE
)

postlasso_pred_1s <- predict(
  sl_post_lasso, 
  data.frame(
    treatmentZRR = rep(0, nrow(W2)), 
    W2, W2 * 0
    )[, c(kept_variables, kept_variables2)], 
  onlySL = T)


cate_postlasso <- postlasso_pred_1s$pred - postlasso_pred_0s$pred



# Honest Causal Trees
tree_fml <- as.formula(paste("Y", paste(names(W1), collapse = ' + '), sep = " ~ "))

honest_tree <- honest.causalTree(formula = tree_fml,
                                 data = data.frame(Y=Y1, W1),
                                 treatment = X1,
                                 est_data = data.frame(Y=Y2, W2),
                                 est_treatment = X2,
                                 split.alpha = 0.5,
                                 split.Rule = "CT",
                                 split.Honest = T,
                                 cv.alpha = 0.5,
                                 cv.option = "CT",
                                 cv.Honest = T,
                                 split.Bucket = T,
                                 bucketNum = 5,
                                 bucketMax = 100, # maximum number of buckets
                                 minsize = 50) # number of observations in treatment and control on leaf

honest_tree_prune <- prune(honest_tree, cp = honest_tree$cp[which.min(honest_tree$cp[, 4]) , 1])
cate_honesttree <- predict(honest_tree_prune, newdata = data.frame(Y=Y2, W2), type = "vector")


# Causal forests 
causalforest <- causalForest(tree_fml,
                             data=data.frame(Y=Y1, W1), 
                             treatment=X1, 
                             split.Rule="CT", 
                             split.Honest=T,  
                             split.Bucket=T, 
                             bucketNum = 5,
                             bucketMax = 100, 
                             cv.option="CT", 
                             cv.Honest=T, 
                             minsize = 2, 
                             split.alpha = 0.5, 
                             cv.alpha = 0.5,
                             sample.size.total = floor(nrow(Y1) / 2), 
                             sample.size.train.frac = .5,
                             mtry = ceiling(ncol(W1)/3), 
                             nodesize = 5, 
                             num.trees = 10, 
                             ncov_sample = ncol(W1), 
                             ncolx = ncol(W1))
cate_causalforest <- predict(causalforest, newdata = data.frame(Y=Y2, W2), type = "vector")

```

```{r}
## Compare Heterogeneity
het_effects <- data.frame(ols = cate_ols, 
                     post_selec_lasso = cate_postlasso, 
                     causal_tree = cate_honesttree, 
                     causal_forest = cate_causalforest)

# Set range of the x-axis
xrange <- range( c(het_effects[, 1], het_effects[, 2], het_effects[, 3], het_effects[, 4]))

# Set the margins (two rows, three columns)
par(mfrow = c(2, 4))

# Create a data frame for ggplot
het_effects_long <- melt(het_effects, variable.name = "Method", value.name = "Heterogeneity_Effect")

# Map original method names to custom titles
het_effects_long$Method <- factor(het_effects_long$Method, 
                                  levels = c("ols", "post_selec_lasso", "causal_tree", "causal_forest"),
                                  labels = c("Panel A: OLS", 
                                             "Panel B: Post-selection Lasso", 
                                             "Panel C: Causal tree", 
                                             "Panel D: Causal forest"))

# Calculate mean values for each method
means <- aggregate(Heterogeneity_Effect ~ Method, data = het_effects_long, mean)

# Create the histograms with mean annotation using ggplot2
p <- ggplot(het_effects_long, aes(x = Heterogeneity_Effect, fill = Method)) +
  geom_histogram(color = "black", bins = 30) +
  facet_wrap(~ Method, scales = "free") +
  theme_minimal() +
  labs(x = "Effect Size",
       y = "Frequency") +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12),
    strip.text = element_text(size = 12, face = "bold"),
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Set3") +
  geom_text(data = means, aes(label = paste0("Mean: ", round(Heterogeneity_Effect, 3)), 
                              x = -Inf, y = Inf), 
            hjust = -0.1, vjust = 2, size = 4, color = "black")




# Save the plot to a file
ggsave(paste0(path_figures, "heterogeneity_effects.png"), p, width = 10, height = 6)

rm(list = setdiff(ls(), keep_vars))

```


## Figure D1: Map and distribution of the running variable (no epicenter)

```{r}

# Load the shape file into a dataframe
dfShape <- st_read("/Users/ilanpargamin/Desktop/thesis/THESIS_REPRO/DATA/communes-20220101-shp/communes-20220101.shp") %>%
  select(geometry, insee) %>%
  mutate(codecommune = sub("^0+", "", as.character(insee))) %>%
  select(-insee)

# Load the no-epicenter data
dfMap <- read_excel(paste0(path_data, "dataGeoRDDnoEpicenter1.xlsx")) %>%
  select(codecommune, treatment, distance_to_border) %>%
  mutate(codecommune = sub("^0+", "", codecommune))

dfMap <- merge(dfMap, dfShape, on="codecommune")

dfMap <- st_as_sf(dfMap)


# Update var if necessary
var <- "x"

# Categorize 'distance_to_border'
dfMap$distance_to_border[is.na(dfMap$distance_to_border)] <- 65000
dfMap$x <- cut(dfMap$distance_to_border, breaks = c(-60000, -5000, 5000, 60000, 200000),
               labels = c( "-60 - -5", "-5 - 5", "5-60", "above 60"), include.lowest = TRUE)


# Plot the original map with highlighted treatment areas
map <- ggplot(dfMap) +
  geom_sf(aes(fill = as.factor(!!sym(var)), geometry = geometry), color = NA) +  # Fill color by distance_to_border categories
  scale_fill_manual(values = c("black", "gray30", "gray60", "grey90")) +  # Specify grayscale colors for each category
  #geom_sf(data = treatment_union_sf, fill = NA, color = "black", size = 0.01, lwd = 0.05) +
  theme_minimal() +
  theme(legend.position = "bottom") +  # Move legend to the bottom
  guides(fill = guide_legend(title = "Distance Category (km)")) 


```

```{r }
bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfMap, distance_to_border >= lower_bound & distance_to_border <= upper_bound)


p_dis <- ggplot(df_rdd, 
       aes(x = distance_to_border, fill = factor(treatment))) +
  geom_histogram(color = "white", boundary = 0, bins=100) +
  scale_fill_manual(values = c("gray80", "gray20"), labels = c("Below 0: In program", "Above 0: Not in program")) +
  labs(x = "Distance to program frontier (meter)", y = "Number of localities") +
  theme_minimal() +
  theme(legend.position = c(0.8, 0.8),  # Adjust the position inside the graph
        legend.title = element_blank(),  # Remove legend title
        legend.background = element_rect(fill = "white", color = "black"),  # Optional: add a border around the legend
        legend.box.background = element_rect(color = "black"))  # Optional: add a border around the legend box


combined_plot <- map / p_dis + plot_layout(ncol = 1)

file_path <- paste0(path_figures, "distanceMapAndDistribution_epi.png")

# Save the plot as a PNG file
ggsave(file_path, plot = combined_plot, device = "png", width = 10, height = 12)

```

```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Figure D2: RDD: balancing checks with alternative distance measure
```{r}
if (file.exists(paste0(path_data, "script_sharp_noEpicenter.RData"))) {
  load(paste0(path_data, "script_sharp_noEpicenter.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)

df_rdd <- df_rdd[!duplicated(df_rdd$codecommune), ]
```

```{r}
labels <- c("FN1988"= "Vote share for FN in 1988",
            "pchom" = "Unemployed (%)", 
            "pop" = "Population", 
            "ratEmp" = "In the labor force (%)", 
            "ratForeigners" = "Foreigners (%)", 
            "asso" = "OPI per 1000 inhabitants", 
            "educNoDiplomaPerK" = "No diploma (%)", 
            "educSUPPerK" = "Academic (%)", 
            "educBACPerK" = "Highschool (%)", 
            "educCAPBEPPerK" = "Technical (%)", 
            "poph" = "Ages 20-40, men", 
            "popf" = "Ages 20-40, women", 
            "pagri" = "Agriculture (%)", 
            "pindp" = "Independant (%)", 
            "ppint" = "Intermediate occupations (%)", 
            "pempl" = "Clerical (%)", 
            "pouvr" = "Manual (%)", 
            "altitude" = "Altitude", 
            "superficie" = "Area", 
            "logVac" = "Vacant housing (%)", 
            "haie" = "Fences per squared km", 
            "vigne" = "Vines per squared km", 
            "revenuPerK" = "Taxable income per capita",
            "delta_pop_1982_1990" = "Population change in p.p. 1982-1990", 
            "delta_emp_1982_1990" = "Change in p.p. in the labor force 1982-1990", 
            "min_distance_to_agglo" = "Distance to closest agglomeration (meters)")

# Split controls into two parts
controls_part1 <- controls[1:ceiling(length(controls)/2)]
controls_part2 <- controls[(ceiling(length(controls)/2) + 1):length(controls)]
controls_part1 <- setdiff(controls_part1, c("border", "pop", "FN1988"))
controls_part2 <- setdiff(controls_part2, c("typologie", "pagri", "superficie"))

conditional_on <- c("z", "x", controls, "factor(dep)")


# Normalize the variables
df_rdd_nor <- df_rdd %>%
  mutate(across(all_of(controls_part1), ~ scale(.) %>% as.vector)) %>%
  mutate(across(all_of(controls_part2), ~ scale(.) %>% as.vector))


create_plots_cond <- function(controls_subset, file_name, b) {
  plots <- list()
  df_filtered <- filter(df_rdd_nor, x >= -b & x <= b)
  
  for (y in controls_subset) {
    print(y)
    # Fit regression model and extract residuals
    formula <- as.formula(paste(y, "~", paste(setdiff(conditional_on, y), collapse = " + ")))
    model <- lm(formula, data = df_filtered)
    df_filtered$residuals <- residuals(model)
    
    # Compute clustered standard errors at the "canton" level
    clustered_se <- vcovCL(model, cluster = ~ canton)
    clustered_se <- coeftest(model, vcov = clustered_se)
    # Extract coefficient, clustered standard error, and p-value for `zTRUE`
    z_coeff <- summary(model)$coefficients["zTRUE", "Estimate"]
    z_se <- clustered_se["zTRUE", "Std. Error"]
    z_pvalue <- clustered_se["zTRUE", "Pr(>|t|)"]
  
    # Determine significance stars based on p-value
    significance <- ifelse(z_pvalue < 0.001, "***", 
                           ifelse(z_pvalue < 0.01, "**", 
                                  ifelse(z_pvalue < 0.05, "*", "")))
    
    # Format the text for annotation
    annotation_text <- paste0("Coef. Treatment: ", round(z_coeff, 3), " (", round(z_se, 3), ") ", significance)
    
    
    # Create plot with residuals
    p <- ggplot(df_filtered, aes(x = x, y = residuals)) +
      stat_summary_bin(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                       fun = "mean",  # Calculates the mean for each bin
                       bins = 15,     # Adjust the number of bins as needed
                       geom = "point", size = 2, color = "black") +
      geom_smooth(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                  method = "lm", formula = y ~ poly(x, 1), size = 1, color = "black") +
      geom_vline(xintercept = 0, color = "black") +
      labs(title = labels[y]) +  # Use the label as the title
      theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(size = 50, hjust = 0.5),
          axis.title.x = element_blank(),  # Remove x-axis title
          axis.title.y = element_blank()) +  # Remove y-axis title
    annotate("text", x = Inf, y = Inf, label = annotation_text, hjust = 1.1, vjust = 1.5, size = 6, color = "black", parse = FALSE)
  
    plots[[y]] <- p
  }
  
  # Combine plots into one figure
  png(file_name, width = 800 * 3, height = 900 * ceiling(length(controls_subset) / 3))
  
  combined_plot <- grid.arrange(grobs = plots, ncol = 3, 
                                bottom = textGrob("Distance to Program Frontier", gp = gpar(fontsize = 50)),
                                left = textGrob("Residuals", rot = 90, gp = gpar(fontsize = 50)))
  
  dev.off()
}


# Create and save the plots for each part
create_plots_cond(controls_part1, paste0(path_figures, "RDD_plots_conditional_part1_noEpi.png"), 10000)
create_plots_cond(controls_part2, paste0(path_figures, "RDD_plots_conditional_part2_noEpi.png"), 10000)

```



```{r}

rm(list = setdiff(ls(), keep_vars))

```


## Figure D3: Vote Share for FN in 1988 (placebo test)
```{r}
if (file.exists(paste0(path_data, "script_sharp_noEpicenter.RData"))) {
  load(paste0(path_data, "script_sharp_noEpicenter.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)


df_rdd <- df_rdd[!duplicated(df_rdd$codecommune), ]
```

```{r }

y <- "FN1988"
b <- 10000
  
df_filtered <- filter(df_rdd, x >= -b & x <= b)
  
# Fit regression model and extract residuals
conditional_on <- c("z", "x", controls, "factor(dep)")

formula <- as.formula(paste(y, "~", paste(conditional_on, collapse = " + ")))
model <- lm(formula, data = df_filtered)
df_filtered$residuals <- residuals(model)
    

# Compute clustered standard errors at the "canton" level
clustered_se <- coef_test(model, cluster = df_filtered$canton, vcov = "CR2")


# Extract coefficient, clustered standard error, and p-value for `zTRUE`
z_coeff <- summary(model)$coefficients["zTRUE", "Estimate"]
z_se <- clustered_se["zTRUE", "SE"]
z_pvalue <- clustered_se["zTRUE", "p_Satt"]

# Determine significance stars based on p-value
significance <- ifelse(z_pvalue < 0.001, "***", 
                       ifelse(z_pvalue < 0.01, "**", 
                              ifelse(z_pvalue < 0.05, "*", "")))

# Format the text for annotation
annotation_text <- paste0("Coef. Treatment: ", round(z_coeff, 3), " (", round(z_se, 3), ") ", significance)


# Create plot with residuals
p <- ggplot(df_filtered, aes(x = x, y = residuals)) +
  stat_summary_bin(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                   fun = "mean",  # Calculates the mean for each bin
                   bins = 15,     # Adjust the number of bins as needed
                   geom = "point", size = 2, color = "black") +
  geom_smooth(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
              method = "lm", formula = y ~ poly(x, 1), size = 1, color = "black") +
  geom_vline(xintercept = 0, color = "black") +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 30, hjust = 0.5),
        axis.title.x = element_blank(),  # Remove x-axis title
        axis.title.y = element_blank()) + # Remove y-axis title
  annotate("text", x = Inf, y = Inf, label = annotation_text, hjust = 1.1, vjust = 1.5, size = 6, color = "black", parse = FALSE)


# Combine plots into one figure with unique x and y labels
file_path <- paste0(path_figures, "RDD_plots_conditional_FN1988_noEpi.png")

png(file_path, width = 800, height = 900)

combined_plot <- grid.arrange(p, ncol = 1, 
                              bottom = textGrob("Distance to Program Frontier", gp = gpar(fontsize = 20)),
                              left = textGrob("Residuals", rot = 90, gp = gpar(fontsize = 20)))

dev.off()

```

```{r }
rm(list = setdiff(ls(), keep_vars))
```




## Figure D4: RDD: main outcomes
```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
# df_rdd$deltaFN2002_1988 <- df_rdd$y # voteShareFN2002 or deltaFN2002_1988 or voteShareFN1988
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)

```

```{r }
df_rdd$FN2002 <- df_rdd$y

# Define the labels
labels <- c("FN2002" = "FN vote share in 2002",
            "RPR2002" = "RPR vote share in 2002",
            "turnout_2002" = "Turnout in 2002",
            "FN2007" = "FN vote share in 2007",
            "FN2012" = "FN vote share in 2012")

conditional_on <- c(controls, "dep")

create_plots_cond <- function(controls_subset, file_name, b) {
  plots <- list()
  df_filtered <- filter(df_rdd, x >= -b & x <= b)
  
  for (y in controls_subset) {
    # Fit regression model and extract residuals
    formula <- as.formula(paste(y, "~", paste(conditional_on, collapse = " + ")))
    model <- lm(formula, data = df_filtered)
    df_filtered$residuals <- residuals(model)
    
    # Create plot with residuals in black and white
    p <- ggplot(df_filtered, aes(x = x, y = residuals)) +
      stat_summary_bin(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                       fun = "mean",  # Calculates the mean for each bin
                       bins = 15,     # Adjust the number of bins as needed
                       geom = "point", size = 1.5, color = "black") +
      geom_smooth(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                  method = "lm", formula = y ~ poly(x, 1), size = 1, color = "black") +
      geom_vline(xintercept = 0, color = "black") +
      labs(title = labels[y]) +  # Use the label as the title
      theme_minimal() +
      theme(legend.position = "none",
            plot.title = element_text(size = 50, hjust = 0.5),
            axis.title.x = element_blank(),  # Remove x-axis title
            axis.title.y = element_blank())  # Remove y-axis title
    
    plots[[y]] <- p
  }
  
  # Combine plots into one figure with unique x and y labels
  png(file_name, width = 800 * 3, height = 900 * ceiling(length(controls_subset) / 3))
  
  combined_plot <- grid.arrange(grobs = plots, ncol = 3, 
                                bottom = textGrob("Distance to Program Frontier", gp = gpar(fontsize = 20)),
                                left = textGrob("Residuals", rot = 90, gp = gpar(fontsize = 20)))
  
  dev.off()
}

# Example usage
create_plots_cond(c("FN2002", "RPR2002", "turnout_2002", "FN2007", "FN2012"), paste0(path_figures, "RDD_plots_conditional_outcomes 20000 _noEpi.png"), 10000)
create_plots_cond(c("FN2002", "RPR2002", "turnout_2002", "FN2007", "FN2012"), paste0(path_figures, "RDD_plots_conditional_outcomes 10000 _noEpi.png"), 20000)

rm(df_rdd, dfDistance, dfZRR1995Controls, controls, pattern, threshold, upper_bound, y, conditional_on, labels, create_plots_cond)

```


## Figure F1: RDD: ZRR effect on 1995 FN Vote share


```{r }
if (file.exists(paste0(path_data, "script_sharp_1995.RData"))) {
  load(paste0(path_data, "script_sharp_1995.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
df_rdd$treatmentZRR <- df_rdd$z
df_rdd$pop <- log(df_rdd$pop)


b_values <- c(20000, 10000, 5000)

df_rdd$FN1995 <- df_rdd$y

# Define the labels
labels <- c("FN1995" = "FN vote share in 1995")

conditional_on <- c(controls, "dep")
letters <- c(`20000`="A", `10000`="B", `5000`="C")

create_plots_cond <- function(controls_subset, file_name_prefix, b_values) {
  plots <- list()
  
  for (b in b_values) {
    df_filtered <- subset(df_rdd, x >= -b & x <= b)
    
    for (y in controls_subset) {
      # Fit regression model and extract residuals
      formula <- as.formula(paste(y, "~", paste(conditional_on, collapse = " + ")))
      model <- lm(formula, data = df_filtered)
      df_filtered$residuals <- residuals(model)
      
      # Create plot with residuals in black and white
      p <- ggplot(df_filtered, aes(x = x, y = residuals)) +
        stat_summary_bin(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                         fun = "mean",
                         bins = 25,
                         geom = "point", size = 1.5, color = "black") +
        geom_smooth(aes(group = ifelse(x <= 0, "Inside Program", "Outside Program")),
                    method = "lm", formula = y ~ poly(x, 2), size = 1, color = "black") +
        geom_vline(xintercept = 0, color = "black") +
        labs(title = paste0("Panel ", letters[as.character(b)], " - Bandwidth: ", b/1000, "km")) + 
        theme_minimal() +
        theme(legend.position = "none",
              plot.title = element_text(size = 24, hjust = 0.5),
              axis.title.x = element_blank(),
              axis.title.y = element_blank()
              )
      
      plots[[paste(y, b)]] <- p
    }
  }
  
  # Save combined plots
  png(paste0(file_name_prefix, ".png"), width = 800 * 3, height = 900 * ceiling(length(plots) / 3))
  
  combined_plot <- grid.arrange(grobs = plots, ncol = 3, 
                                bottom = textGrob("Distance to Program Frontier", gp = gpar(fontsize = 20)),
                                left = textGrob("Residuals", rot = 90, gp = gpar(fontsize = 20)))
  
  dev.off()
}


create_plots_cond(c("FN1995"), paste0(path_figures, "RDD_plots_conditional_outcomes1995.png"), b_values)



rm(list = setdiff(ls(), keep_vars))

```

# Tables
## Table 1: Descriptive Statistics in 1990
```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}

dataNames <- setdiff(names(df_merged), c("codecommune", "nomcommune", "nom", "year_treat", "canton", "dep", "year", "reg", "typologie"))

# Filter the data for the year 
y <- 1990
df <- dfZRRControls %>% filter(year == y)
df <- df[c(dataNames, "codecommune")]

# merge with dfZRR to get year_treat
df <- df %>% left_join(dfZRR %>% select(codecommune, year_treat) %>% unique(), 
                       by=c("codecommune"))


# add Haie et Vigne

dfHaie <- dfHaie %>%
  distinct(codecommune, .keep_all = TRUE)

df <- df %>% left_join(dfHaie %>% select(codecommune, haie, vigne), 
                       by=c("codecommune"))

# add revenue
df <- df %>% left_join(dfRevenue %>% filter(year==1994) %>% select(codecommune, revenuPerK), 
                       by=c("codecommune"))

dataNames <- c(dataNames, "haie", "vigne", "revenuPerK")

# Convert only numeric columns to numeric
df[dataNames] <- lapply(df[dataNames], as.numeric)

# Replace infinite values with NA
for (var in dataNames) {
  df <- df %>%
    mutate(across(all_of(var), ~ ifelse(is.infinite(.), NA, .)))
}

# rename "delta_pop_1982_1990", "delta_emp_1982_1990", "min_distance_to_agglo", 
df <- df %>% 
  rename(deltaPop19821990 = delta_pop_1982_1990) %>%
  rename(deltaEmp19821990 = delta_emp_1982_1990) %>%
  rename(minDistToAgglo = min_distance_to_agglo) 

dataNames <- setdiff(dataNames, c("delta_pop_1982_1990", "delta_emp_1982_1990", "min_distance_to_agglo"))
dataNames <- c(dataNames, "deltaPop19821990", "deltaEmp19821990", "minDistToAgglo")

# fill na
df <- df %>%
  mutate(year_treat = ifelse(is.na(year_treat), 0, year_treat))


# Create the groups based on year_treat
df <- df %>%
  mutate(group = case_when(
    year_treat == 1995 ~ "Treated_in_1995",
    year_treat == 0 ~ "Never_treated",
    year_treat >1995 ~ "Treated_after_1995"
  ))

df <- df %>% filter(!is.na(group))

group_counts <- df %>%
  group_by(group) %>%
  summarise(n = n()) 
group_counts <- as.data.frame(lapply(group_counts, as.character))



# Define the list of variables to be multiplied by 100
percentage_vars <- c("pchom", "ratEmp", "ratForeigners", "educNoDiplomaPerK", 
                     "educSUPPerK", "educBACPerK", "educCAPBEPPerK", 
                     "poph", "popf", "pagri", "pindp", "ppint", "pempl", "pouvr", "logVac")

# Multiply the relevant columns by 100
df <- df %>% 
  mutate(across(all_of(percentage_vars), ~ . * 100, .names = "{.col}"))



```

```{r}
# Summarize the data for each group
summary_table <- df %>%
  group_by(group) %>%
  summarise(across(all_of(c(dataNames)), 
                   list(mean = ~ mean(.x, na.rm = TRUE), sd = ~ sd(.x, na.rm = TRUE)), 
                   .names = "{col}_{fn}")) %>%
  pivot_longer(-group, names_to = c("variable", ".value"), names_sep = "_")  %>%
  pivot_wider(names_from = group, values_from = c(mean, sd))

# Round the mean and SD values
summary_table <- summary_table %>%
  mutate(across(starts_with("mean_"), ~ round(., 2))) %>%
  mutate(across(starts_with("sd_"), ~ round(., 2)))

# Rearranging  columns
summary_table <- summary_table %>%
  select(variable, 
         mean_Treated_in_1995, sd_Treated_in_1995,
         mean_Treated_after_1995, sd_Treated_after_1995,
         mean_Never_treated, sd_Never_treated)


# Calculate group counts
group_counts <- df %>%
  group_by(group) %>%
  summarise(n = n())

# Convert all columns in summary_table to character type to ensure compatibility
summary_table <- summary_table %>%
  mutate(across(everything(), as.character))

# Create a data frame with group counts
counts_df <- data.frame(
  variable = "Observations",
  mean_Treated_in_1995 = as.character(group_counts$n[group_counts$group == "Treated_in_1995"]),
  sd_Treated_in_1995 = "",
  mean_Treated_after_1995 = as.character(group_counts$n[group_counts$group == "Treated_after_1995"]),
  sd_Treated_after_1995 = "",
  mean_Never_treated = as.character(group_counts$n[group_counts$group == "Never_treated"]),
  sd_Never_treated = ""
)


# Append the counts to the summary table
summary_table <- bind_rows(summary_table, counts_df)

# Renaming columns
colnames(summary_table) <- c("Variable", 
                             "Treated in 1995 (Mean)", "Treated in 1995 (SD)",
                             "Treated after 1995 (Mean)", "Treated after 1995 (SD)",
                             "Never Treated (Mean)", "Never Treated (SD)")



# Update the labels
labels <- c("Unemployed (%)", 
            "Population", "In the labor force (%)", "Foreigners (%)", 
            "OPI per 1,000 inhabitants", "No diploma (%)", "Academic (%)", 
            "Highschool (%)", "Technical (%)", "Ages 20-40 (%), men", 
            "Ages 20-40 (%), women", "Agriculture (%)", "Independant (%)", 
            "Intermediate occupations (%)", "Clerical (%)", "Manual workers (%)", "Altitude", 
            "Area (km^2)", "Vacant housing (%)", "Fences per km^2", "Vines per km^2", "Taxable income per capita (log)",
            "Population change in p.p. 1982-1990", "Labor force change in p.p. 1982-1990", 
            "Distance to closest agglomeration (km)", "Observations")


# Ensure the labels match the number of rows
if(length(labels) != nrow(summary_table)) {
  stop("The number of labels does not match the number of rows in the summary table.")
}


# Assign the labels to the Variable column
summary_table$Variable <- labels

labels_ordered <- c("Unemployed (%)", "In the labor force (%)", "Agriculture (%)", "Independant (%)", "Intermediate occupations (%)", "Clerical (%)", "Manual workers (%)", "Labor force change in p.p. 1982-1990", 
            "Population",  "Foreigners (%)", "Ages 20-40 (%), men", "Ages 20-40 (%), women",  "Vacant housing (%)", "Population change in p.p. 1982-1990", "OPI per 1,000 inhabitants", "Taxable income per capita (log)",
             "No diploma (%)", "Academic (%)", "Highschool (%)", "Technical (%)", 
            "Altitude", "Distance to closest agglomeration (km)", "Area (km^2)",  "Fences per km^2", "Vines per km^2", 
             "Observations")


# Reorder rows in summary_table based on the labels
summary_table <- summary_table %>% 
  arrange(factor(Variable, levels = labels_ordered))


# Insert section headers directly into summary_table
summary_table <- summary_table %>%
  add_row(Variable = "Employment", .before = 1) %>%
  add_row(Variable = "Demographics", .before = 10) %>%
  add_row(Variable = "Education", .before = 19) %>%
  add_row(Variable = "Geography", .before = 24)

# Ensure that the section header rows have empty values for all other columns
summary_table <- summary_table %>%
  mutate(across(-Variable, ~ ifelse(Variable %in% c("Employment", "Demographics", "Education", "Geography"), "", .)))

# Format numbers with commas and two decimal places
summary_table_formatted <- summary_table %>%
  mutate(across(where(is.numeric), ~ formatC(.x, format = "f", digits = 2, big.mark = ",")))


# Export summary_table to a nicely formatted LaTeX table
kable(summary_table, format = "latex", booktabs = TRUE, 
      col.names = c("Variable", "Mean", "SD", "Mean", "SD", "Mean", "SD"), 
      caption = "Descriptive Statistics in 1990", 
      label = "descriptive") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  add_header_above(c(" " = 1, 
                     "Treated in 1995" = 2, 
                     "Treated after 1995" = 2, 
                     "Never Treated" = 2)) %>%
  row_spec(which(summary_table$Variable %in% c("Employment", "Demographics", "Education", "Geography", "Observations")), bold = TRUE) %>%
  footnote(general = "Entries are descriptive statistics for the most important variables in my locality-level data set. Mean and standard deviation values are reported separately. For precise definitions and the sources of all variables, see the Data Section.",
           threeparttable = TRUE) %>%
  save_kable(paste0(path_tables, "table1.tex"))


kable(summary_table, booktabs = TRUE, 
      col.names = c("Variable", "Mean", "SD", "Mean", "SD", "Mean", "SD"), 
      caption = "Descriptive Statistics in 1990", 
      label = "descriptive") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  add_header_above(c(" " = 1, 
                     "Treated in 1995" = 2, 
                     "Treated after 1995" = 2, 
                     "Never Treated" = 2)) %>%
  row_spec(which(summary_table$Variable %in% c("Employment", "Demographics", "Education", "Geography", "Observations")), bold = TRUE) %>%
  footnote(general = "Entries are descriptive statistics for the most important variables in my locality-level data set. Mean and standard deviation values are reported separately. For precise definitions and the sources of all variables, see the Data Section.",
           threeparttable = TRUE)


rm(list = setdiff(ls(), keep_vars))
path_data <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/DATA/"
path_out <- "/Users/ilanpargamin/Desktop/ECONOMICS/thesis/THESIS_REPRO/OUTPUT/"
path_figures <- paste0(path_out, "figures/")
path_tables <- paste0(path_out, "tables/")

```



## Table 2: Preliminary evidence: estimated effect of the ZRR program
```{r}

if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}
dfZRRControls <- dfZRRControls %>% filter(year_treat > 0)
dfZRRControls <- dfZRRControls %>%
    filter(!is.na(FN) ) %>%
  filter(!is.na(treatment) )

dfZRRControls <- dfZRRControls %>%
  # Calculate the time since treatment
  mutate(time_since_open = year - year_treat) 

# We keep the years 1988 and 2002 when we have elections results
dfSpe <- dfZRRControls  %>%
  filter(year %in% list(1988,2002) )

# create treated var
dfSpe$treated = ifelse( dfSpe$year_treat == 1995, 1, 0)
# Create a dummy variable to indicate the time when the treatment started. Lets assume that treatment started in 1995. In this case, years before 1995 will have a value of 0 and 1995+ a 1. 

dfSpe$post = ifelse(dfSpe$year >= 1995, 1, 0)

controls <- names(dfSpe)
controls <- setdiff(controls, c("codecommune", "nomcommune", "dep", "year", "reg", "nom", "codecommune", "time_since_open", "year_treat", "post", "FN_log", "treated", "FN", "treatment", "FN1995", "RPR", "deltaFN", "turnout_2002", "canton"))

variables_to_clean <- c("codecommune", "FN", "year", controls, "post", "treated",  "year_treat", "time_since_open", "canton", "reg", "dep")

# Loop through each variable in the list
for(var in variables_to_clean) {
  # Remove rows with NA values in the current variable
  dfSpe <- dfSpe[!is.na(dfSpe[[var]]), ]
  
  # Remove rows with Inf or -Inf values in the current variable
  dfSpe <- dfSpe[!is.infinite(dfSpe[[var]]), ]
}

dfSpe <- dfSpe %>%
  select(all_of(variables_to_clean))

dfSpe$did <- dfSpe$post * dfSpe$treated

dfSpe <- dfSpe %>%
  distinct()

# DID
formula <- as.formula(paste("FN ~ post + treated + did + ", paste(controls, collapse = " + ")))
did_model1 <- lm(formula , 
                data = dfSpe)

# Within model
panel <- pdata.frame(dfSpe, "codecommune")
did_model2 <- plm(formula , 
                data = panel, model = "within")

# Clustered standard errors at the "canton" level
did_model1_se <- coeftest(did_model1, vcovCL(did_model1, type = "HC1", cluster = dfSpe$canton))
did_model2_se <- coeftest(did_model2, vcovHC(did_model2, type = "HC1", cluster = "group"))


```


```{r }


if (file.exists(paste0(path_data, "allpop.RData"))) {
  load(paste0(path_data, "allpop.RData"))
} else {
  message("Data environment does not exist")
}

df_merged1 <- df_merged1 %>%
  # Calculate the time since treatment
  mutate(time_since_open = year - year_treat) 

# We keep the years 1988 and 2002 when we have elections results
dfSpe <- df_merged1  %>%
  filter(year %in% list(1988,2002) )

dfSpe$post = ifelse(dfSpe$year >= 1995, 1, 0)

controls <- names(dfSpe)
controls <- setdiff(controls, c("codecommune","codecommune1", "nomcommune", "year", "reg", "nom", "codecommune", "time_since_open", "year_treat", "post", "FN_log", "treated", "FN", "treatment", "FN1995", "RPR", "deltaFN", "turnout_2002", "canton", "revenuPerK"))

variables_to_clean <- c("codecommune1", "FN", "year", controls, "post", "treatment")

# Loop through each variable in the list
for(var in variables_to_clean) {
  # Remove rows with NA values in the current variable
  dfSpe <- dfSpe[!is.na(dfSpe[[var]]), ]
  
  # Remove rows with Inf or -Inf values in the current variable
  dfSpe <- dfSpe[!is.infinite(dfSpe[[var]]), ]
}

dfSpe <- dfSpe %>%
  select(all_of(c(variables_to_clean,  "year_treat", "time_since_open")))

dfSpe$year_treat[is.na(dfSpe$year_treat)] <- "never"
dfSpe$year_treat <- as.factor(dfSpe$year_treat)

dfSpe$did <- dfSpe$post * dfSpe$treatment

dfSpe <- dfSpe %>%
  distinct()

# Create dummy variables
dummy_vars <- model.matrix(~ year_treat - 1, data = dfSpe)

# Convert to data frame and rename columns
dummy_vars <- as.data.frame(dummy_vars)
colnames(dummy_vars) <- paste0("year_treat_", levels(dfSpe$year_treat))

# Combine the dummy variables with the original data frame
dfSpe <- cbind(dfSpe, dummy_vars)

# Create a new variable that is 1 if treatment year is after 2004, and 0 otherwise
dfSpe$year_treat_after_2004 <- ifelse(dfSpe$year_treat %in% c("2005", "2006", "2007", "2009", "2010", "2013", "2014", "2017", "2018"), 1, 0)

# Define columns to keep
cols_to_keep <- c("year_treat_1995", "year_treat_never", "year_treat_after_2004")

# Select only the columns in cols_to_keep, along with all other columns not starting with "year_treat_"
dfSpe <- dfSpe %>%
  select(all_of(cols_to_keep), everything()) %>%
  select(-starts_with("year_trea"), all_of(cols_to_keep), everything())




# DID
formula <- as.formula(paste("FN ~ year_treat_1995 + year_treat_never + post*year_treat_1995 + post*year_treat_never  + ", paste(controls, collapse = " + ")))
did_model3 <- lm(formula , 
                data = dfSpe)


models <- list(did_model1, did_model2, did_model3)

# Clustered standard errors at the "canton" level
did_model3_se <- coeftest(did_model3, vcovCL(did_model3, type = "HC1", cluster = dfSpe$canton))

# Extract standard errors for stargazer
clustered_se <- list(did_model1_se[, "Std. Error"], did_model2_se[, "Std. Error"], did_model3_se[, "Std. Error"])

# Create stargazer table with clustered standard errors
stargazer(models, 
          type = "text", 
          se = clustered_se, # Insert clustered SEs
          omit = controls, 
          add.lines = list(c("Controls", "True", "True", "True")),
          dep.var.labels = "The vote share for FN in the 2002 presidential elections",
          model.names = TRUE,
          #column.labels = c("OLS", "Within", "OLS", "Within"),
          notes = "The OLS estimate of the effect of the ZRR program on the vote share for FN in the 2002 presidential elections is of the same magnitude as that of the within estimator: it decreases the vote share by 1 percentage point. Since the first difference estimator is the same as the within one in a two-period setting, we only report the within estimator. Standard errors are reported at the county level.",
          title = "Preliminary evidence: estimated effect of the ZRR program",
          label = "tab:did_result",
          out = paste0(path_tables, "table2.tex"))


rm(list = setdiff(ls(), keep_vars))

```

### As in paper
```{r}
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}
dfZRRControls <- dfZRRControls %>% filter(year_treat > 0)
dfZRRControls <- dfZRRControls %>%
    filter(!is.na(FN) ) %>%
  filter(!is.na(treatment) )


# We keep the years 1988 and 2002 when we have elections results
dfSpe <- dfZRRControls  %>%
  filter(year %in% list(1988,2002) ) %>%
  mutate(
    post = ifelse(year >= 1995, 1, 0),
    did = post * treatment,
    time_since_open = year - year_treat
  )

# Define control variables
controls <- names(dfSpe)
controls <- setdiff(controls, c("codecommune", "codecommune1", "nomcommune", "year", "reg", "dep", "nom", "time_since_open",
                                "year_treat", "post", "FN_log", "treated", "FN", "treatment", "FN1995",
                                "RPR", "deltaFN", "turnout_2002", "canton", "revenuPerK", "did"))

# Clean data: remove rows with NA or infinite in relevant variables
variables_to_clean <- c("codecommune", "FN", "year", "post", "treatment", controls, "canton", "reg")
for (var in variables_to_clean) {
  dfSpe <- dfSpe[!is.na(dfSpe[[var]]) & !is.infinite(dfSpe[[var]]), ]
}

# Final variable selection
dfSpe <- dfSpe %>%
  select(all_of(c("codecommune", "FN", "year", "post", "treatment", "dep", "canton", controls))) %>%
  distinct()

# Turn data into panel data
panel <- pdata.frame(dfSpe, "codecommune")

# First Difference / Between: no controls
formula1 <- as.formula(paste("FN ~ post:treatment"))
did_model1 <- plm(formula1 , 
                data = panel, model = "fd")

# First Difference / Between: + controls
formula2 <- as.formula(paste("FN ~ post:treatment +", paste(controls, collapse = " + ")))
did_model2 <- plm(formula2 , 
                data = panel, model = "fd")

# First Difference / Between: + controls _ dep
formula3 <- as.formula(paste("FN ~ post:treatment + post:dep +", paste(controls, collapse = " + ")))
did_model3 <- plm(formula3 , 
                data = panel, model = "fd")

# OLS: + controls + FE
formula <- as.formula(paste("FN ~ post:treatment  + ", paste(controls, collapse = " + "), "+ factor(dep)"))
did_model3 <- lm(formula , 
                data = dfSpe)

# Clustered SEs at canton level
se_clustered1 <- coef_test(did_model1, cluster = panel$canton, vcov = "CR2")
se_clustered2 <- coef_test(did_model2, cluster = panel$canton, vcov = "CR2")
se_clustered3 <- coeftest(did_model3, vcovCL(did_model3, type = "HC1", cluster = dfSpe$canton))


stargazer(list(did_model1, did_model2, did_model3),
          type = "text",
          #se = list(se_clustered1[, "SE"], se_clustered2[, "SE"]), did_model3_se[, "SE"]),
          omit = c(controls), "factor\\(dep\\)", "factor\\(post\\)"),
          add.lines = list(c("Controls", "No", "Yes", "Yes")),
          dep.var.labels = "Vote share for FN (2002)",
          model.names = TRUE,
          title = "Effect of the ZRR Program on FN Vote Share (2002)",
          label = "tab:did_result",
          out = paste0(path_tables, "table2.tex"),
          keep.stat = c("n", "rsq"),
omit.stat = c("adj.rsq", "ser", "f"),
          notes = "All models are estimated using a first-difference approach between 1988 and 2002. Standard errors are clustered at the county (canton) level.")

# Stargazer summary table
stargazer(list(did_model1, did_model2), #, did_model3),
          type = "text",
          se = list(se_clustered1[, "SE"], se_clustered2[, "SE"]),#, did_model3_se[, "SE"]),
          omit = c(controls),#, "factor\\(dep\\)"),
          add.lines = list(c("Controls", "No", "Yes", "Yes")),
          dep.var.labels = "Vote share for FN (2002)",
          model.names = TRUE,
          title = "Effect of the ZRR Program on FN Vote Share (2002)",
          label = "tab:did_result",
          out = paste0(path_tables, "table2.tex"),
          keep.stat = c("n", "rsq"),
omit.stat = c("adj.rsq", "ser", "f"),

          notes = "All models are estimated using a first-difference approach between 1988 and 2002. Standard errors are clustered at the county (canton) level.")

```

## Table 3: Summary Statistics for Different Bandwidths
```{r}
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}
```

```{r }

bandwidths <- c(20000, 10000, 5000) # it corresponds to the distance (in meters)

controls <- setdiff(controls, c("border", "typologie"))

# Define a function to compute the required statistics for a given dataframe
compute_statistics <- function(df) {
  nb_canton <- df %>% distinct(canton, dep) %>% nrow()
  nb_communes <- df %>% nrow()
  avg_communes_per_canton <- nb_communes / nb_canton
  avg_pop_per_canton <- sum(df$pop, na.rm = TRUE) / nb_canton
  
  stats <- df %>%
    group_by(canton) %>%
    summarize(sd_pop = sd(pop, na.rm = TRUE),
              min_pop = min(pop, na.rm = TRUE),
              max_pop = max(pop, na.rm = TRUE)) %>%
    summarize(avg_sd_pop = mean(sd_pop, na.rm = TRUE),
              avg_min_pop = mean(min_pop, na.rm = TRUE),
              avg_max_pop = mean(max_pop, na.rm = TRUE))
  
  stats <- stats %>%
    mutate(nb_canton = nb_canton,
           avg_communes_per_canton = avg_communes_per_canton,
           avg_pop_per_canton = avg_pop_per_canton)
  
  return(stats)
}


# Define a function to compute statistics for control (z=0) and treatment (z=1) groups within a specific bandwidth
compute_bandwidth_statistics <- function(b) {
  df <- subset(dfZRR1995Controls, x >= -b & x <= b) 
  df0 <- df %>% filter(z == 0)
  df1 <- df %>% filter(z == 1)
  
  control_stats <- compute_statistics(df0)
  treatment_stats <- compute_statistics(df1)
  
  # Combine the control and treatment statistics for the bandwidth
  stats <- data.frame(
    Statistic = c("Number of cantons", "Average communes per canton", "Average pop per canton",
                  "Average SD of pop", "Average Min of pop", "Average Max of pop"),
    Control = c(control_stats$nb_canton, control_stats$avg_communes_per_canton, control_stats$avg_pop_per_canton,
                control_stats$avg_sd_pop, control_stats$avg_min_pop, control_stats$avg_max_pop),
    Treatment = c(treatment_stats$nb_canton, treatment_stats$avg_communes_per_canton, treatment_stats$avg_pop_per_canton,
                  treatment_stats$avg_sd_pop, treatment_stats$avg_min_pop, treatment_stats$avg_max_pop)
  )
  
  return(stats)
}

# Loop over bandwidths and compute statistics for each one
all_results <- lapply(bandwidths, compute_bandwidth_statistics)

# Combine results into a single data frame for easier export
results <- do.call(cbind, lapply(seq_along(bandwidths), function(i) {
  setNames(all_results[[i]][, -1], paste0(names(all_results[[i]][, -1]), "_", bandwidths[i]))
}))

# Add the Statistic column as the first column
results <- cbind(Statistic = all_results[[1]]$Statistic, results)

# Same but for controls
# Function to compute mean and standard deviation for each column in controls
compute_statistics_controls <- function(df, controls) {
  summary_stats <- data.frame(
    Variable = controls,
    Mean = sapply(df[controls], mean, na.rm = TRUE),
    SD = sapply(df[controls], sd, na.rm = TRUE)
  )
  return(summary_stats)
}

# Function to compute statistics for control and treatment groups within a specified bandwidth
compute_bandwidth_statistics_controls <- function(b, dfZRR1995Controls, controls) {
  # Filter data within the bandwidth
  df <- subset(dfZRR1995Controls, x >= -b & x <= b)
  df <- df[, c(controls, "z")]
  
  # Separate control and treatment groups
  df0 <- subset(df, z == 0)
  df1 <- subset(df, z == 1)
  
  # Compute statistics for control and treatment groups
  control_stats <- compute_statistics_controls(df0, controls)
  treatment_stats <- compute_statistics_controls(df1, controls)
  
  # Combine the control and treatment statistics for each variable
  stats <- data.frame(
    Statistic = control_stats$Variable,
    Control = control_stats$Mean,
    #Control_SD = control_stats$SD,
    Treatment = treatment_stats$Mean#,
    #Treatment_SD = treatment_stats$SD
  )
  
  return(stats)
}

labels <- c("FN1988"= "Vote share for FN in 1988",
            "pchom" = "Unemployed (%)", 
            "delta_pop_1982_1990"= "Population change in p.p. 1982-1990",
            "delta_emp_1982_1990"= "Employment change in p.p. 1982-1990",
            "pop" = "Population", 
            "ratEmp" = "Employed (%)", 
            "ratForeigners" = "Foreigners (%)", 
            "asso" = "OPI per 1000 inhabitants", 
            "educNoDiplomaPerK" = "No diploma (%)", 
            "educSUPPerK" = "Academic education (%)", 
            "educBACPerK" = "Highschool education (%)", 
            "educCAPBEPPerK" = "Technical education (%)", 
            "poph" = "Proportion of 20-40, men", 
            "popf" = "Proportion of 20-40, women", 
            "pagri" = "Agriculture workers (%)", 
            "pindp" = "Independant workers (%)", 
            "ppint" = "Intermediate occupations (%)", 
            "pempl" = "Clerical workers (%)", 
            "pouvr" = "Manual workers (%)", 
            "altitude" = "Altitude", 
            "superficie" = "Locality size", 
            "logVac" = "Proportion of vacant housing (log)", 
            "haie" = "Fences per km2", 
            "vigne" = "Vines per km2", 
            "revenuPerK" = "Taxable income per capita",
            "min_distance_to_agglo" = "Distance to closest agglomeration")


# Loop over bandwidths and compute statistics for each one
compute_all_bandwidth_statistics <- function(bandwidths, dfZRR1995Controls, controls, labels) {
  all_results <- lapply(bandwidths, function(b) compute_bandwidth_statistics_controls(b, dfZRR1995Controls, controls))
  
  # Replace variable names with labels in each result
  for (i in seq_along(all_results)) {
    all_results[[i]]$Statistic <- labels[all_results[[i]]$Statistic]
  }
  
  # Combine results into a single data frame for easier export
  results <- do.call(cbind, lapply(seq_along(bandwidths), function(i) {
    setNames(all_results[[i]][, -1], paste0(names(all_results[[i]][, -1]), "_", bandwidths[i]))
  }))
  
  # Add the Statistic column as the first column with labels
  results <- cbind(Statistic = all_results[[1]]$Statistic, results)
  
  return(results)
}

# Usage example
results_controls <- compute_all_bandwidth_statistics(bandwidths, dfZRR1995Controls, controls, labels)

results_out <- rbind(results, results_controls)


labels_ordered <- c("Number of cantons", "Average communes per canton", "Average pop per canton",
                  "Average SD of pop", "Average Min of pop", "Average Max of pop", 
                  "Vote share for FN in 1988",
                  "Unemployed (%)", "Employed (%)", "Agriculture workers (%)", "Independant workers (%)", "Intermediate occupations (%)", "Clerical workers (%)", "Manual workers (%)", "Employment change in p.p. 1982-1990", 
            "Population",  "Foreigners (%)", "Proportion of 20-40, men", "Proportion of 20-40, women",  "Proportion of vacant housing (log)", "Population change in p.p. 1982-1990", "OPI per 1000 inhabitants", 
             "No diploma (%)", "Academic education (%)", "Highschool education (%)", "Technical education (%)", 
            "Altitude", "Distance to closest agglomeration", "Locality size",  "Fences per km2", "Vines per km2")




results_out_ordered <- results_out[match(labels_ordered, results_out$Statistic), ]


# Display the results using stargazer
stargazer(results_out, type = "latex", summary = FALSE,
          title = "Summary Statistics for Different Bandwidths",
          digits = 2,
          out = paste0(path_tables, "table3.tex"),
          rownames = FALSE,
          label = "tab:summary_stats_combined",
          notes="The table displays the main summary statistics of the demographic distributions of the sample as well as the summary statistics of the main controls for each bandwidth, with separate columns for the Control (C) and Treatment (T) groups. "
          )




rm(list = setdiff(ls(), keep_vars))
```

## Table 4: Main results, different bandwidths
```{r }
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```

```{r }
b1 <- 20000
b2 <- 10000
b3 <- 5000

df_rdd$x_km <- df_rdd$x / 1000
df_rdd$x_km10 <- df_rdd$x / 10000

formula <- as.formula(paste("y ~ z + x_km10 + ", paste(controls, collapse = " + "), "+ factor(dep) "))


model_bw_1 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b1 &
                                 x <= b1))

cluster_se_bw_1 <- coeftest(model_bw_1, vcov = vcovHC(model_bw_1, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))


model_bw_2 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b2 &
                                 x <= b2))

cluster_se_bw_2 <- coeftest(model_bw_2, vcov = vcovHC(model_bw_2, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))

model_bw_3 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b3 &
                                 x <= b3))

cluster_se_bw_3 <- coeftest(model_bw_3, vcov = vcovHC(model_bw_3, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))


model_names <- c(paste0("Bandwidth = ", b1), 
                 paste0("Bandwidth = ", b2), 
                 paste0("Bandwidth = ", b3))
models <- list( model_bw_1, model_bw_2, model_bw_3)

names(models) <- model_names


modelsummary(models, 
             vcov = list(cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3),  # Use the clustered standard errors
                          estimate = "{estimate} ({std.error}){stars}",  # Correct formatting
             stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001),  # Customize significance levels if needed
             fmt =4,
              notes = c("Standard errors are clustered at the canton level"),  # Optional explanatory note
             statistic = c(),
             gof_omit = "IC|Log|Adj|p\\.value|statistic|se_type|Std.Errors|RMSE",
             output = "kableExtra"
) %>%
  row_spec(2, background = "#F5ABEA") %>%
  kable_styling()




# Create a function to check if terms are included in the formula
has_fe <- function(model, term) {
  term %in% attr(model$terms, "term.labels")
}

# Prepare the summary table
stargazer(models, 
          type = "latex", 
          column.labels = model_names, 
          covariate.labels = c("Treatment ZRR", "Distance to Frontier (10 km)"), 
          omit = c("factor\\(dep\\)", controls), 
          add.lines = list(
            c("Controls", rep(TRUE, length(models))),  # Assuming controls are included in all models
            c("Department fixed effects", sapply(models, function(m) has_fe(m, "factor(dep)")))
          ), 
          omit.stat = c("LL", "ser", "f"), 
          digits = 4,
          #se = list(cluster_se, cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3), 
          star.cutoffs = c(0.05, 0.01, 0.001), 
          notes = "Standard errors are clustered at the canton level",
          out=paste0(path_tables, "table4.tex"),
          title="Main results, different bandwidths",
          label="tab:rdd_results_diffbandwidth"
)


```



```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table 5: Main results when bandwidth is 10 km, different specifications
```{r }
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```

```{r }

b <- 10000


# Initialize a list to store the regression models
models <- list()

df_rdd$log_pop_density <- log(df_rdd$pop / df_rdd$superficie)

# Define the groups of control variables
control_groups <- list(
  group1 = list(vars = c("x"), dep = FALSE, controls = FALSE),
  group2 = list(vars = c("x", "pop", "superficie"), dep = FALSE, controls = FALSE),
  group3 = list(vars = c("x", "pop", "superficie", "dep"), dep = TRUE, controls = FALSE),
  group4 = list(vars = c("x", "pop", "superficie", setdiff(controls, c("pop", "superficie"))), dep = FALSE, controls = TRUE),
  group6 = list(vars = c("x", "pop", "superficie", setdiff(controls, c("pop", "superficie")), "dep"), dep = TRUE, controls = TRUE)
)

df_filtered <- filter(df_rdd, x >= -b & x <= b)
df_filtered$x <- df_filtered$x / 1000

# Run regressions with grouped control variables and store the models
for (i in seq_along(control_groups)) {
  control_vars <- control_groups[[i]]$vars
  formula <- as.formula(paste("y ~ treatmentZRR +", paste(control_vars, collapse = " + ")))
  models[[paste0("group", i)]] <- lm(formula, data = df_filtered)
}

# Create lines indicating the inclusion of fixed effects and controls
fe_lines <- list(
  "Dept FE" = sapply(control_groups, function(x) ifelse(x$dep, "True", "False")),
  "Controls" = sapply(control_groups, function(x) ifelse(x$controls, "True", "False"))
)

# Generate the regression table using stargazer
stargazer(models, type = "text",
          title = "Main results when bandwidth is 10 km, different specifications",
          dep.var.labels = "y",
          covariate.labels = c("treatmentZRR", "x"),
          omit = c(setdiff(controls, "superficie"), "dep"),  # Omit all control variables
          omit.stat = c("adj.rsq", "ser", "f"),
          add.lines = list(
            c("Controls", fe_lines$`Controls`),
            c("Dept FE", fe_lines$`Dept FE`)
          ),
          star.cutoffs = c(0.05, 0.01, 0.001), 
          column.sep.width = "3pt",
          float = FALSE,
          notes="I restrict the sample to the municipalities located 10km at most from the frontier program.",
          out=paste0(path_tables, "table5.tex"),
          tabel="tab:rdd_results_10")

```


```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table 6: RDD main specification results on other outcomes
```{r }
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```


```{r}
df_rdd$FN2002 <- df_rdd$y

formulas <- list(
  deltaFN = paste("deltaFN ~ z + x + border +", paste(controls, collapse = " + "), "+ factor(dep) "), 
  RPR2002 = paste("RPR2002 ~ z + x + border +", paste(controls, collapse = " + "), "+ factor(dep) "),
  FN1988 = paste("FN1988 ~ z + x + border +", paste(controls, collapse = " + "), "+ factor(dep) "),
  turnout_2002 = paste("turnout_2002 ~ z + x + border +", paste(controls, collapse = " + "), "+ factor(dep) "),
  FN2007 = paste("FN2007 ~ z + x + border +", paste(controls, collapse = " + "), " + factor(dep) "), 
  FN2012 = paste("FN2012 ~ z + x + border +", paste(controls, collapse = " + "), " + factor(dep)"),
  FN2017 = paste("FN2017 ~ z + x + border +", paste(controls, collapse = " + "), " + factor(dep)"),
  FN2022 = paste("FN2022 ~ z + x + border +", paste(controls, collapse = " + "), " + factor(dep) ")
)


# Bandwidth values
b1 <- 20000
b2 <- 10000
b3 <- 5000

bandwidths <- list(b1, b2, b3)

# Initialize an empty list to store the models
all_models <- list()
nobs_vector <- c()

# Fit models for each formula and bandwidth
for (bw in bandwidths) {
  print(bw)
  filtered_data <- filter(df_rdd, x >= -bw & x <= bw)
  nobs_vector <- c(nobs_vector, nrow(filtered_data))
  for (outcome in names(formulas)) {
    print(outcome)
    model <- lm(as.formula(formulas[[outcome]]), data = filtered_data)
    coefs <- coeftest(model, vcov = vcovHC(model, type = "HC1", cluster = "group", cluster.id = filtered_data$canton))
    model_name <- paste(outcome, "bw", bw, sep = "_")
    all_models[[model_name]] <- coefs
  }
}

# Extract the coefficients
extract_coefficients <- function(model, variable) {
  if (variable %in% rownames(model)) {
    coef <- model[variable, "Estimate"]
    se <- model[variable, "Std. Error"]
    stars <- ifelse(model[variable, "Pr(>|t|)"] < 0.001, "***", 
                    ifelse(model[variable, "Pr(>|t|)"] < 0.01, "**", 
                           ifelse(model[variable, "Pr(>|t|)"] < 0.05, "*", "")))
    return(paste0(round(coef, 3), " (", round(se, 3), ")", stars))
  } else {
    return(NA)
  }
}


# Create a summary table
outcome_names <- c("deltaFN", "RPR2002", "FN1988", "turnout_2002", "FN2007", "FN2012", "FN2017", "FN2022")
summary_table <- data.frame(Outcome = outcome_names)


# Fill the summary table with coefficients for "z"
for (bw in bandwidths) {
  column_name <- paste("bw", bw, sep = "_")
  for (outcome in outcome_names) {
    model_name <- paste(outcome, "bw", bw, sep = "_")
    summary_table[summary_table$Outcome == outcome, column_name] <- extract_coefficients(all_models[[model_name]], "zTRUE")
  }
}

# Add a row for number of observations
nobs_row <- data.frame(Outcome = "nobs")
for (i in 1:length(bandwidths)) {
  nobs_row[1, paste("bw", bandwidths[[i]], sep = "_")] <- nobs_vector[i]
}

# Combine the nobs row with the summary table
summary_table <- rbind(summary_table, nobs_row)


# Print the summary table
summary_table %>%
  kable(format = "latex", booktabs = TRUE) %>%
  kable_styling()#  %>%
  # save_kable(file = paste0(path_tables, "table6.tex"))

stargazer(summary_table, 
          type = "latex", style="qje",
          summary = FALSE,
          star.cutoffs = c(0.05, 0.01, 0.001), 
          title = "RDD main specification results on other outcomes",
          label = "tab:rdd_results_outcomes_later",
          out = paste0(path_tables, "table6.tex"),
          notes="I run the specification with controls and place fixed effects. The standard errors are clustered at the county level.")








rm(list = setdiff(ls(), keep_vars))

```



## Table 7: Comparison of Residual Means between Control and Treatment Groups with T-Test Results
```{r }
if (file.exists(paste0(path_data, "borders_pair.RData"))) {
  load(paste0(path_data, "borders_pair.RData"))
} else {
  message("Data environment does not exist")
}

df_rct <- dfZRR1995Controls %>%
  select(-x) %>%
  rename(treatmentZRR=z) 

labels <- c("FN1988"= "Vote share for FN in 1988",
            "pchom" = "Unemployed (%)", 
            "pop" = "Population", 
            "ratEmp" = "In the labor force (%)", 
            "ratForeigners" = "Foreigners (%)", 
            "asso" = "OPI per 1000 inhabitants", 
            "educNoDiplomaPerK" = "No diploma (%)", 
            "educSUPPerK" = "Academic (%)", 
            "educBACPerK" = "Highschool (%)", 
            "educCAPBEPPerK" = "Technical (%)", 
            "poph" = "Ages 20-40, men", 
            "popf" = "Ages 20-40, women", 
            "pagri" = "Agriculture (%)", 
            "pindp" = "Independant (%)", 
            "ppint" = "Intermediate occupations (%)", 
            "pempl" = "Clerical (%)", 
            "pouvr" = "Manual (%)", 
            "altitude" = "Altitude", 
            "superficie" = "Area", 
            "logVac" = "Vacant housing (%)", 
            "haie" = "Fences per squared km", 
            "vigne" = "Vines per squared km", 
            "revenuPerK" = "Taxable income per capita",
            "delta_pop_1982_1990" = "Population change in p.p. 1982-1990", 
            "delta_emp_1982_1990" = "Change in p.p. in the labor force 1982-1990", 
            "min_distance_to_agglo" = "Distance to closest agglomeration (meters)")

```

```{r }
target_vars <- setdiff(controls, c("treatmentZRR", "border_pair",  "typologie"))
df_rct$density <- df_rct$pop / df_rct$superficie

# Define control variables
conditional_on <- c("border_pair", "treatmentZRR", "pagri", "density") # 


# Function to calculate mean and t-test of residuals
calculate_residual_balance <- function(df, grouping_var, target_vars, conditional_on) {
  # Create a list to store residuals for each target variable
  residuals_list <- list()

  # Calculate residuals for each target variable
  for (var in target_vars) {
    formula <- as.formula(paste(var, "~", paste(setdiff(conditional_on, var), collapse = " + ")))
    model <- lm(formula, data = df)
    residuals <- resid(model)
    residuals_list[[var]] <- residuals
    df[[paste0(var, "_resid")]] <- residuals
  }
  
  # Calculate means of residuals
  means <- df %>%
    group_by({{ grouping_var }}) %>%
    summarise(across(ends_with("_resid"), mean, na.rm = TRUE)) %>%
    pivot_longer(-{{ grouping_var }}, names_to = "variable", values_to = "mean") %>%
    mutate(variable = str_remove(variable, "_resid")) %>%
    pivot_wider(names_from = {{ grouping_var }}, values_from = mean, names_prefix = "mean_")
  
  # Calculate t-tests on residuals
  t_tests <- map_dfr(target_vars, function(var) {
    test <- t.test(df[[paste0(var, "_resid")]] ~ df[[rlang::as_string(rlang::ensym(grouping_var))]])
    data.frame(
      variable = var,
      estimate = test$estimate[2] - test$estimate[1],
      p.value = test$p.value
    )
  })
  
  # Merge means and t-tests
  summary <- means %>%
    left_join(t_tests, by = "variable") %>%
    mutate(significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ))
  
  return(summary)
}


# Calculate balance summary using residuals
balance_summary <- calculate_residual_balance(df_rct, treatmentZRR, target_vars, conditional_on)

# Rename columns for better readability
balance_summary <- balance_summary %>%
  select(variable, 
         `Control (mean)` = mean_FALSE, 
         `Treatment (mean)` = mean_TRUE, 
         `Difference` = estimate, 
         `p-value` = p.value)

# Replace variable names with labels
balance_summary$variable <- labels[balance_summary$variable]

# Round the values to 4 digits
balance_summary <- balance_summary %>%
  mutate(
    `Control (mean)` = round(`Control (mean)`, 4),
    `Treatment (mean)` = round(`Treatment (mean)`, 4),
    `Difference` = round(`Difference`, 4),
    `p-value` = round(`p-value`, 4)
  )

# Use stargazer to display the results in LaTeX format
stargazer(
  balance_summary %>%
    select(-Difference),  # Exclude the Difference column
  type = "latex",
  summary = FALSE,
  title = "Balancing tests for border pairs",
  note = "The table displays the means of the residuals of the regression of the variable on the border-pair fixed effects along with the population density and the share of agriculture workers of the locality. The right columns show the significance of the t-test to compare both groups among the border municipalities.",
  digits = 2,
  dep.var.labels.include = FALSE,
  rownames = FALSE,
  label="tab:ttest-border",
  out=paste0(path_tables, "table7.tex")
)


balance_summary

rm(list = setdiff(ls(), keep_vars))

```




## Table 8: Border municipalities: regression Results
```{r }
if (file.exists(paste0(path_data, "borders_pair.RData"))) {
  load(paste0(path_data, "borders_pair.RData"))
} else {
  message("Data environment does not exist")
}

df_rct <- dfZRR1995Controls %>%
  select(-x) %>%
  rename(treatmentZRR=z) 

controls <- controls[!controls %in% c("haie", "vigne")]

```

```{r }

# 1
model1 <- lm(y ~ treatmentZRR, data = df_rct)

# 2
formula <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + ")))
model2 <- lm(formula, data = df_rct)

# 3
formula <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + "), "+ factor(dep)"))
model3 <- lm(formula, data = df_rct %>% filter(same_department == 1))

# 4
formula <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + "), " + factor(border_pair)"))
model4 <- lm(formula, data = df_rct)

# 5
formula <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + "), "+ factor(dep)", " + factor(border_pair)"))
model5 <- lm(formula, data = df_rct)

# 6
df_rct_panel <- pdata.frame(df_rct, index = c("border_pair", "year"))
formula_fd <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + ")))
model6 <- plm(formula_fd, data = df_rct_panel, model = "fd")

# 7 placebo
formula <- as.formula(paste("FN1988 ~ treatmentZRR +", paste(controls[!controls %in% c("FN1988")], collapse = " + "), "+ factor(dep)", " + factor(border_pair)"))
model7 <- lm(formula, data = df_rct)



# Use stargazer to display the results with a note about fixed effects
stargazer(
  model1, model2, model3, model4, model5, model6, model7,
  type = "latex",
  title = "Regression Results: Effect of ZRR on FN Vote Share in 2002 and Robustness Checks",
  star.cutoffs = c(0.05, 0.01, 0.001), 
  label = "tab:border-results",
  omit = c("factor\\\\(dep\\\\)", "factor\\\\(border_pair\\\\)", controls, "Constant"),
  dep.var.labels.include = FALSE,
  column.labels = c("No Controls", "Controls", "Controls + Dept FE", "Controls + Pair FE", "Dept + Pair FE", "First Diff", "Placebo (1988)"),
  add.lines = list(
    c("Controls", "No", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes"),
    c("Department Fixed Effects", "No", "No", "Yes", "No", "Yes", "/", "Yes"),
    c("Border Pair Fixed Effects", "No", "No", "No", "Yes", "Yes", "/", "Yes")
  ),
  digits = 4,
  notes = "The table presents the regression results of the effect of the ZRR program on the vote share for the FN in 2002, using a sample of 11,604 pairs of border municipalities. The placebo column uses FN vote share in 1988 as the outcome. First-difference specification in column (6) captures change between 1988 and 2002.",
  out = paste0(path_tables, "table8.tex")
)

```

```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table 9: Winsorizing, trimming and doughnut: Estimation of Treatment Effect
```{r }
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

df_rdd$x <- df_rdd$x / 1000

```

```{r }
# Define the formula
formula <- as.formula(paste("y ~ z + x + ", paste(controls, collapse = " + "), "+ factor(dep)"))

# Winsorizing
winsorize <- function(x, probs = c(0.01, 0.99)) {
  limits <- quantile(x, probs = probs)
  x[x < limits[1]] <- limits[1]
  x[x > limits[2]] <- limits[2]
  return(x)
}

df_winsorized <- df_rdd %>%
  mutate(across(where(is.numeric), ~ winsorize(.)))

model_winsorized <- lm(formula, data = df_winsorized)

# Trimming
trim <- function(x, probs = c(0.01, 0.99)) {
  limits <- quantile(x, probs = probs)
  x[x < limits[1] | x > limits[2]] <- NA
  return(x)
}

df_trimmed <- df_rdd %>%
  mutate(across(where(is.numeric), ~ trim(.))) %>%
  drop_na()

model_trimmed <- lm(formula, data = df_trimmed)

# Doughnut Approach
doughnut_radius <- 5  # Set the radius you want to remove around the border

df_doughnut <- df_rdd %>%
  filter(abs(x) > doughnut_radius)

model_doughnut <- lm(formula, data = df_doughnut)

# Prepare models for stargazer
models <- list(model_winsorized, model_trimmed, model_doughnut)
model_names <- c("Winsorized", "Trimmed", "Doughnut")

# Create a function to check if terms are included in the formula
has_fe <- function(model, term) {
  term %in% attr(model$terms, "term.labels")
}

# Prepare the summary table
stargazer(models, 
          type = "latex", 
          title="Winsorizing, trimming and doughnut: Estimation of Treatment Effect",
          column.labels = model_names, 
          covariate.labels = c("z", "x"), 
          omit = c("factor\\(dep\\)", controls), 
          add.lines = list(
            c("Controls", rep(TRUE, length(models))),  # Assuming controls are included in all models
            c("Department fixed effects", sapply(models, function(m) has_fe(m, "factor(dep)"))),
            c("Region fixed effects", sapply(models, function(m) has_fe(m, "factor(reg)")))
          ), 
          omit.stat = c("LL", "ser", "f"), 
          label = "tab:robustness-wtd",
          star.cutoffs = c(0.05, 0.01, 0.001), 
          notes = "Winsorizing: I replace the outliers with the 5th and the 95th values. Trimming: I remove the outliers, i.e. the values of the data outside the 5th and 95th percentiles. Doughnut: I remove the observations that are withing a 5km range from the frontier border. Standard errors are clustered at the canton level.",
          out=paste0(path_tables, "table9.tex")
)


```

```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table 10: Main results, different bandwidths - Placebo 


```{r }

if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}
rm(dfDistance)

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

b1 <- 20
b2 <- 10
b3 <- 5

formula <- as.formula(paste("y ~ z + x + ", paste(controls, collapse = " + "), "+ factor(dep)"))

# Helper function to add significance stars based on p-values
add_stars <- function(coef, se) {
  t_value <- abs(coef / se)
  if (t_value > 2.58) {
    return("***")
  } else if (t_value > 1.96) {
    return("**")
  } else if (t_value > 1.65) {
    return("*")
  } else {
    return("")
  }
}


# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Initialize an empty list to store results for each iteration of i
results_list <- list()


# Loop over files of distance data
for (i in 0:10){
  
  # Prepare data
  dfDistance <- read_excel(paste0(path_data, "dataGeoRDD_canton_random/dataGeoRDD_canton_random", i, ".xlsx")) %>%
    select(-year) %>%
    mutate(
      codecommune = sub("^0+", "", as.character(codecommune)),
      x = distance_to_border,
      z = ifelse(x < 0, 1, 0),
      treatment = z,
      distance_to_border = x
    ) %>%
    select(codecommune, x, z, distance_to_border)

  df_rdd <- left_join(dfZRR1995Controls %>% 
                        select(-x, -z) %>%
                        distinct(codecommune, .keep_all = TRUE) %>%
                        mutate(codecommune = sub("^0+", "", as.character(codecommune))), 
                      dfDistance %>% 
                        distinct(codecommune, .keep_all = TRUE), by="codecommune") %>%
    filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
    mutate(
      dist = x,                     
      treatmentZRR = z,              
      pop = log(pop)                 
    ) %>%
    distinct(codecommune, .keep_all = TRUE)  %>%
    select(-dep) %>%                      
    left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
    mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
    select(-codecanton)                    # Remove codecanton after renaming
  df_rdd$x <- df_rdd$x / 1000
  
  # Temporary storage for this iteration's results
  iter_results <- data.frame(
    Bandwidth = model_names,
    Coefficient = NA,
    Clustered_SE = NA
  )

  # Loop through each bandwidth model
  for (j in 1:3) {
    bw <- get(paste0("b", j))
    model <- lm(formula, data = filter(df_rdd, x >= -bw & x <= bw))
    
    # Extract coefficient and clustered standard error for zTRUE, Store in results matrix
    iter_results[j, "Coefficient"]  <- coef(model)["z"]
    iter_results[j, "Clustered_SE"] <- sqrt(diag(vcovHC(model, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))["z"])
  }
  
  # Add results of this iteration to the list
  results_list[[paste0("Iteration_", i)]] <- iter_results
  
}

# Combine all results into a single data frame
final_results <- do.call(rbind, lapply(names(results_list), function(name) {
  cbind(Iteration = name, results_list[[name]])
}))


# Format results with coefficient, standard error, and significance stars
formatted_results <- final_results %>%
  mutate(
    Significance = mapply(add_stars, Coefficient, Clustered_SE),
    Formatted = sprintf("%.3f (%.3f)%s", Coefficient, Clustered_SE, Significance)
  ) %>%
  select(Iteration, Bandwidth, Formatted) %>%
  pivot_wider(
    names_from = Bandwidth,
    values_from = Formatted
  ) %>%
  rename(`Bandwidth 20` = `Bandwidth = 20`, `Bandwidth 10` = `Bandwidth = 10`, `Bandwidth 5` = `Bandwidth = 5`)

# Display the formatted table in LaTeX
tab <- kbl(formatted_results, format = "latex", booktabs = TRUE, 
           caption = "Regression Results: Coefficient and Clustered SE for treatment effect of ZRR",
           label="rdd_results_diffbandwidth_placebo") %>%
  kable_styling(latex_options = c("hold_position", "scale_down")) %>%
  add_header_above(c("Iteration" = 1, "Bandwidth 20" = 1, "Bandwidth 10" = 1, "Bandwidth 5" = 1)) %>%
  row_spec(0, bold = TRUE)

cat(tab, file = paste0(path_tables, "table10.tex"))




```



```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table 11: The top 10% and bottom 10% of the treatment effect distribution according to the Causal Forest Model
```{r }
if (file.exists(paste0(path_data, "borders_pair.RData"))) {
  load(paste0(path_data, "borders_pair.RData"))
} else {
  message("Data environment does not exist")
}


controls <- setdiff(controls, c("typologie", "x"))

df_rct <- dfZRR1995Controls %>%
  select(-x, -typologie) %>%
  select(c("y", "z", "canton", "dep",  "border_pair", controls)) %>%
  rename(treatmentZRR=z)  

my_data <- df_rct[complete.cases(df_rct), ]
my_data$pop <- log(my_data$pop)
my_data$treatmentZRR <- as.numeric(my_data$treatmentZRR)
my_data[] <- lapply(names(my_data), function(x) {
  if (x != "border_pair") {
    as.numeric(my_data[[x]])
  } else {
    my_data[[x]]
  }
})

df <- my_data
rm(my_data, df_rct)

set.seed(142)


# Split data into 3 samples
folds = createFolds(1:nrow(df), k=2)

Y1 <- df[folds[[1]],1]
Y2 <- df[folds[[2]],1]

X1 <- df[folds[[1]],2]
X2 <- df[folds[[2]],2]

W1 <- df[folds[[1]], controls]
W2 <- df[folds[[2]], controls]

### Creates a vector of 0s and a vector of 1s of length n (hack for later usage)
zeros <- function(n) {
  return(integer(n))
}
ones <- function(n) {
  return(integer(n)+1)
}


# Rename the columns using the provided labels
labels <- c("FN1988"= "FN vote share in 1988",
            "pchom" = "unemployed (%)", 
            "pop" = "population", 
            "ratEmp" = "employed (%)", 
            "ratForeigners" = "foreigners (%)", 
            "asso" = "OPI per 1000 inhabitants", 
            "educNoDiplomaPerK" = "no diploma (%)", 
            "educSUPPerK" = "academic education (%)", 
            "educBACPerK" = "highschool education (%)", 
            "educCAPBEPPerK" = "technical education (%)", 
            "poph" = "proportion of 20-40, men", 
            "popf" = "proportion of 20-40, women", 
            "pagri" = "agriculture workers (%)", 
            "pindp" = "independent workers (%)", 
            "ppint" = "intermediate occupations (%)", 
            "pempl" = "clerical workers (%)", 
            "pouvr" = "manual workers (%)", 
            "altitude" = "altitude", 
            "superficie" = "locality size", 
            "logVac" = "proportion of vacant housing (log)", 
            "haie" = "fences per km2", 
            "vigne" = "vines per km2", 
            "revenuPerK" = "Taxable income per capita",
            "delta_pop_1980_1995" = "population change in p.p. 1982-1990", 
            "delta_emp_1980_1990" = "employed change in p.p. 1982-1990", 
            "min_distance_to_agglo" = "distance to closest agglomeration")

tree_fml <- as.formula(paste("Y", paste(names(W1), collapse = ' + '), sep = " ~ "))
causalforest <- causalForest(tree_fml,
                             data=data.frame(Y=Y1, W1), 
                             treatment=X1, 
                             split.Rule="CT", 
                             split.Honest=T,  
                             split.Bucket=T, 
                             bucketNum = 5,
                             bucketMax = 100, 
                             cv.option="CT", 
                             cv.Honest=T, 
                             minsize = 2, 
                             split.alpha = 0.5, 
                             cv.alpha = 0.5,
                             sample.size.total = floor(nrow(Y1) / 2), 
                             sample.size.train.frac = .5,
                             mtry = ceiling(ncol(W1)/3), 
                             nodesize = 5, 
                             num.trees = 10, 
                             ncov_sample = ncol(W1), 
                             ncolx = ncol(W1))

# Step 1: Predict the treatment effects
cate_causalforest <- predict(causalforest, newdata = data.frame(Y=Y2, W2), type = "vector")

# Step 2: Identify the bottom 10% and top 10% groups
threshold_bottom <- quantile(cate_causalforest, 0.1)
threshold_top <- quantile(cate_causalforest, 0.9)

strong_effect_subsample_bottom <- W1[cate_causalforest <= threshold_bottom, ]
strong_effect_subsample_top <- W1[cate_causalforest >= threshold_top, ]

# Step 3: Conduct t-tests for the main variables between the two groups
t_test_results <- lapply(1:ncol(strong_effect_subsample_bottom), function(i) {
  t.test(strong_effect_subsample_bottom[, i], strong_effect_subsample_top[, i])
})

# Extract the t-test results for each variable
t_test_summary <- data.frame(
  Variable = names(labels)[names(labels) %in% colnames(strong_effect_subsample_bottom)],
  #t_value = sapply(t_test_results, function(x) x$statistic),
  p_value = sapply(t_test_results, function(x) x$p.value),
  mean_bottom = sapply(t_test_results, function(x) x$estimate[1]),
  mean_top = sapply(t_test_results, function(x) x$estimate[2])
)

# Step 4: Add significance stars based on p-values
t_test_summary$Significance <- cut(t_test_summary$p_value,
                                   breaks = c(-Inf, 0.01, 0.05, 0.1, Inf),
                                   labels = c("***", "**", "*", ""))


# Add the human-readable labels as a separate column
t_test_summary$Description <- labels[t_test_summary$Variable]

# Reorder columns to put the description first
t_test_summary <- t_test_summary[, c("Description", "p_value", "Significance", "mean_bottom", "mean_top")]

# Step 4: Display the t-test results using stargazer
stargazer(t_test_summary, type = "latex", summary = FALSE, digits = 3,
          out = paste0(path_tables, "table11.tex"),
          title="Comparison of Heterogeneity Effects",
          label="fig:heterogeneity",
          notes="The table presents a comparison between the top 10\ and bottom 10\ of the treatment effect distribution as estimated by the Causal Forest Model. The columns display the mean values of key socioeconomic and demographic variables for both groups, the t-value from a t-test comparing the means, the corresponding p-value, and significance levels indicated by stars (*** p < 0.01, ** p < 0.05, * p < 0.1). This comparison highlights the characteristics of the groups most and least affected by the treatment.",
          rownames = FALSE)


rm(list = setdiff(ls(), keep_vars))
```
## Table 12: Effect of the ZRR program on the 1999 socioeconomic variables
```{r }
if (file.exists(paste0(path_data, "eco_outcomes.RData"))) {
  load(paste0(path_data, "eco_outcomes.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

lower_bound <- threshold - bandwidth
upper_bound <- threshold + bandwidth

df_rdd <- subset(dfZRR1995Controls, x >= lower_bound & x <= upper_bound)
rm(lower_bound, bandwidth)

df_rdd$dist <- df_rdd$x
df_rdd$treatmentZRR <- df_rdd$z

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

```

```{r}
# Initialize an empty list to store the models and number of observations
all_models <- list()
nobs_vector <- c()

upper_bound <- 20000
b1 <- 10000
b2 <- 5000

bandwidths <- list(upper_bound, b1, b2)
outcomes <- setdiff(c(outcomes, "log_pop"), c("vigne", "haie", "min_distance_to_agglo", "pop"))
dfZRR1995Outcomes$log_pop_1999 <- log(dfZRR1995Outcomes$pop_1999)
outcomes_2002 <- paste0(outcomes, "_", year_outcome)

# Loop over each outcome
for (y in outcomes_2002) {
  print(y)
  # Merge df_rdd with dfZRR1995Outcomes to get the outcome values
  df_rdd1 <- df_rdd %>%
    left_join(dfZRR1995Outcomes %>% select(codecommune, all_of(y)), by = "codecommune")
  df_rdd1 <- unique(df_rdd1)
  # Create formula for the current outcome
  formula <- paste(y, " ~ z + x +", paste(controls, collapse = " + "), "+ factor(dep)")
  
  # Fit models for each bandwidth
  for (bw in bandwidths) {
    filtered_data <- filter(df_rdd1, x >= -bw & x <= bw)
    nobs_vector <- c(nobs_vector, nrow(filtered_data))
    
    # Run the regression
    model <- lm(as.formula(formula), data = filtered_data)
    coefs <- coeftest(model, vcov = vcovHC(model, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))
    
    # Save the model with a specific name
    model_name <- paste(y, "bw", bw, sep = "_")
    all_models[[model_name]] <- coefs
  }
}

# Function to extract coefficients
extract_coefficients <- function(model, variable) {
  if (variable %in% rownames(model)) {
    coef <- model[variable, "Estimate"]
    se <- model[variable, "Std. Error"]
    stars <- ifelse(model[variable, "Pr(>|t|)"] < 0.001, "***", 
                    ifelse(model[variable, "Pr(>|t|)"] < 0.01, "**", 
                           ifelse(model[variable, "Pr(>|t|)"] < 0.05, "*", "")))
    return(paste0(round(coef, 3), " (", round(se, 3), ")", stars))
  } else {
    return(NA)
  }
}

# Create a summary table
summary_table <- data.frame(Outcome = paste0(outcomes, "_", year_outcome))

# Fill the summary table with coefficients for "z"
for (bw in bandwidths) {
  column_name <- paste("bw", bw, sep = "_")
  for (outcome in paste0(outcomes, "_", year_outcome)) {
    model_name <- paste(outcome, "bw", bw, sep = "_")
    summary_table[summary_table$Outcome == outcome, column_name] <- extract_coefficients(all_models[[model_name]], "zTRUE")
  }
}

# Add a row for number of observations
nobs_row <- data.frame(Outcome = "nobs_1999")
for (i in 1:length(bandwidths)) {
  nobs_row[1, paste("bw", bandwidths[[i]], sep = "_")] <- nobs_vector[i]
}

# Combine the nobs row with the summary table
summary_table <- rbind(summary_table, nobs_row)

labels <- c("FN1988"= "Vote share for FN in 1988",
            "pchom" = "Unemployed (%)", 
            "log_pop" = "Population (log)", 
            "ratEmp" = "Employed (%)", 
            "ratForeigners" = "Foreigners (%)", 
            "asso" = "OPI per 1000 inhabitants", 
            "educNoDiplomaPerK" = "No diploma (%)", 
            "educSUPPerK" = "Academic (%)", 
            "educBACPerK" = "Highschool (%)", 
            "educCAPBEPPerK" = "Technical (%)", 
            "poph" = "Ages 20-40 (%), men", 
            "popf" = "Ages 20-40 (%), women", 
            "pagri" = "Agriculture (%)", 
            "pindp" = "Independant (%)", 
            "ppint" = "Intermediate occupations (%)", 
            "pempl" = "Clerical (%)", 
            "pouvr" = "Manual (%)", 
            "altitude" = "Altitude", 
            "superficie" = "Area", 
            "logVac" = "Vacant housing (%)", 
            "haie" = "Fences per squared km", 
            "vigne" = "Vines per squared km", 
            "revenuPerK" = "Taxable income per capita",
            "delta_pop_1982_1990" = "Population change in p.p. 1982-1990", 
            "delta_emp_1982_1990" = "Employed change in p.p. 1982-1990", 
            "min_distance_to_agglo" = "Distance to closest agglomeration",
            "nobs" ="Observations")

names(labels) <- paste0(names(labels), "_1999")

summary_table$Outcome <- labels[summary_table$Outcome]

kable_output <- summary_table %>%
  kable(format = "latex", booktabs = TRUE) %>%
  kable_styling()

save_kable(kable_output, file = paste0(path_tables, "table12.tex"))
```


```{r }
rm(list = setdiff(ls(), keep_vars))
```
## Table 13: ZRR effect on absolute number of votes (log) for FN, different bandwidths
```{r }
if (file.exists(paste0(path_data, "script_sharp.RData"))) {
  load(paste0(path_data, "script_sharp.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```

```{r }
df_rdd$FN2002abs <- log(df_rdd$y * exp(df_rdd$pop) * df_rdd$turnout_2002 + 0.0001)



b1 <- 20000
b2 <- 10000
b3 <- 5000

formula <- as.formula(paste("FN2002abs ~ z + x + ", paste(controls, collapse = " + "), " + factor(dep) "))


model_bw_1 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b1 &
                                 x <= b1))

cluster_se_bw_1 <- coeftest(model_bw_1, vcov = vcovHC(model_bw_1, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))


model_bw_2 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b2 &
                                 x <= b2))

cluster_se_bw_2 <- coeftest(model_bw_2, vcov = vcovHC(model_bw_2, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))

model_bw_3 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b3 &
                                 x <= b3))

cluster_se_bw_3 <- coeftest(model_bw_3, vcov = vcovHC(model_bw_3, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))


model_names <- c(paste0("Bandwidth = ", b1), 
                 paste0("Bandwidth = ", b2), 
                 paste0("Bandwidth = ", b3))
models <- list( model_bw_1, model_bw_2, model_bw_3)

names(models) <- model_names


modelsummary(models, 
             vcov = list(cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3),  # Use the clustered standard errors
                          estimate = "{estimate} ({std.error}){stars}",  # Correct formatting
             stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001),  # Customize significance levels if needed
              notes = c("Standard errors are clustered at the canton level"),  # Optional explanatory note
             statistic = c(),
             gof_omit = "IC|Log|Adj|p\\.value|statistic|se_type|Std.Errors|RMSE",
             output = "kableExtra"
) %>%
  row_spec(2, background = "#F5ABEA") %>%
  kable_styling()




# Create a function to check if terms are included in the formula
has_fe <- function(model, term) {
  term %in% attr(model$terms, "term.labels")
}

# Prepare the summary table
stargazer(models, 
          type = "latex", 
          column.labels = model_names, 
          covariate.labels = c("Treatment ZRR", "Distance to Frontier"), 
          omit = c("factor\\(canton\\)", "factor\\(dep\\)", "factor\\(reg\\)", controls), 
          add.lines = list(
            c("Controls", rep(TRUE, length(models))),  # Assuming controls are included in all models
            c("Department fixed effects", sapply(models, function(m) has_fe(m, "factor(dep)"))),
            c("Region fixed effects", sapply(models, function(m) has_fe(m, "factor(reg)")))
          ), 
          omit.stat = c("LL", "ser", "f"), 
          #se = list(cluster_se, cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3), 
          star.cutoffs = c(0.05, 0.01, 0.001), 
          notes = "Standard errors are clustered at the canton level",
          out=paste0(path_tables, "table13.tex")
)

```

```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table A1: Evolution of Socioeconomic Variables between 1988 and 2002 with T-Test Results
```{r }
if (file.exists(paste0(path_data, "dataDes.RData"))) {
  load(paste0(path_data, "dataDes.RData"))
} else {
  message("Data environment does not exist")
}

# We keep the years 1988 and 2002 when we have elections results
dfSpe <- dfZRRControls  %>%
  filter(year %in% list(1988,2002) )

# create treated var
dfSpe$treated = ifelse( dfSpe$year_treat == 1995, 1, 0)
# Create a dummy variable to indicate the time when the treatment started. Lets assume that treatment started in 1995. In this case, years before 1995 will have a value of 0 and 1995+ a 1. 

dfSpe$post = ifelse(dfSpe$year >= 1995, 1, 0)

controls <- names(dfSpe)
controls <- setdiff(controls, c("codecommune", "nomcommune", "dep", "year", "reg", "nom", "codecommune", "time_since_open", "year_treat", "post", "FN_log", "treated", "FN", "treatment", "FN1995", "RPR", "deltaFN", "turnout_2002", "canton"))

variables_to_clean <- c("codecommune", "FN", "year", controls, "post", "treated")

# Loop through each variable in the list
for(var in variables_to_clean) {
  # Remove rows with NA values in the current variable
  dfSpe <- dfSpe[!is.na(dfSpe[[var]]), ]
  
  # Remove rows with Inf or -Inf values in the current variable
  dfSpe <- dfSpe[!is.infinite(dfSpe[[var]]), ]
}

dfSpe <- dfSpe %>%
  select(all_of(variables_to_clean))

dfSpe$did <- dfSpe$post * dfSpe$treated

dfSpe <- dfSpe %>%
  distinct()

```


```{r }
labels <- c("FN" = "FN vote share",
            "pchom" = "unemployed (%)", 
  "pop" = "population size", 
  "ratEmp" = "employed (%)", 
  "ratForeigners" = "foreigners (%)", 
  "asso" = "OPI per 1000 inhabitants", 
  "educNoDiplomaPerK" = "no diploma (%)", 
  "educSUPPerK" = "academic education (%)", 
  "educBACPerK" = "highschool education (%)", 
  "educCAPBEPPerK" = "technical education (%)", 
  "poph" = "proportion of 20-40, men", 
  "popf" = "proportion of 20-40, women", 
  "pagri" = "agriculture workers (%)", 
  "pindp" = "independent workers (%)", 
  "ppint" = "intermediate occupations (%)", 
  "pempl" = "clerical workers (%)", 
  "pouvr" = "manual workers (%)", 
  "altitude" = "altitude", 
  "superficie" = "locality size", 
  "logVac" = "proportion of vacant housing (log)", 
  "haie" = "fences per km2", 
  "vigne" = "vines per km2", 
  "revenuPerK" = "Taxable income per capita",
  "delta_pop_1982_1990" = "population change in p.p. 1982-1990", 
  "delta_emp_1982_1990" = "employed change in p.p. 1982-1990", 
  "min_distance_to_agglo" = "distance to closest agglomeration"
)


# Function to calculate mean and t-test
calculate_balance <- function(df, grouping_var, target_vars) {
  # Calculate means
  means <- df %>%
    group_by({{ grouping_var }}) %>%
    summarise(across(all_of(target_vars), mean, na.rm = TRUE)) %>%
    pivot_longer(-{{ grouping_var }}, names_to = "variable", values_to = "mean") %>%
    pivot_wider(names_from = {{ grouping_var }}, values_from = mean, names_prefix = "mean_")
  
  # Calculate t-tests
  t_tests <- map_dfr(target_vars, function(var) {
    test <- t.test(df[[var]] ~ df[[rlang::as_string(rlang::ensym(grouping_var))]])
    data.frame(
      variable = var,
      estimate = test$estimate[2] - test$estimate[1],
      p.value = test$p.value
    )
  })
  
  # Merge means and t-tests
  summary <- means %>%
    left_join(t_tests, by = "variable") %>%
    mutate(significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ))
  
  return(summary)
}


# Calculate balance summary
vars <- setdiff(controls, "typologie")
balance_summary <- calculate_balance(dfSpe, post, c("FN",vars))

# Rename columns for better readability
colnames(balance_summary) <- c("Variable", "1988 (mean)", "2002 (mean)", "Difference", "p-value", "Significance")


# Replace variable names with labels
balance_summary$Variable <- labels[balance_summary$Variable]


# Generate the LaTeX table and assign it to a variable
latex_table <- balance_summary %>%
  kable(format = "latex", 
        caption = "Comparison of Means between Control and Treatment Groups with T-Test Results", 
        digits = c(NA, 4, 4, 4, 3, NA),
        booktabs = TRUE, 
        escape = FALSE) %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), full_width = FALSE, position = "center")

# Save the table to a .tex file
cat(latex_table, file = paste0(path_tables, "tableA1.tex"))



```

```{r }
rm(list = setdiff(ls(), keep_vars))
```
## Table D1: Main results, different bandwidths
```{r }
if (file.exists(paste0(path_data, "script_sharp_noEpicenter.RData"))) {
  load(paste0(path_data, "script_sharp_noEpicenter.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0


# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes


# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)
```

```{r }
b1 <- 20000
b2 <- 10000
b3 <- 5000

df_rdd$x_km <- df_rdd$x / 1000

formula <- as.formula(paste("y ~ z + x_km + ", paste(controls, collapse = " + "), "+ factor(dep) "))


model_bw_1 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b1 &
                                 x <= b1))

cluster_se_bw_1 <- coeftest(model_bw_1, vcov = vcovHC(model_bw_1, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))


model_bw_2 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b2 &
                                 x <= b2))

cluster_se_bw_2 <- coeftest(model_bw_2, vcov = vcovHC(model_bw_2, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))

model_bw_3 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b3 &
                                 x <= b3))

cluster_se_bw_3 <- coeftest(model_bw_3, vcov = vcovHC(model_bw_3, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))



model_names <- c(paste0("Bandwidth = ", b1), 
                 paste0("Bandwidth = ", b2), 
                 paste0("Bandwidth = ", b3))
models <- list( model_bw_1, model_bw_2, model_bw_3)

names(models) <- model_names


modelsummary(models, 
             vcov = list(cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3),  # Use the clustered standard errors
                          estimate = "{estimate} ({std.error}){stars}",  # Correct formatting
             stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001),  # Customize significance levels if needed
              notes = c("Standard errors are clustered at the canton level"),  # Optional explanatory note
             statistic = c(),
             gof_omit = "IC|Log|Adj|p\\.value|statistic|se_type|Std.Errors|RMSE",
             output = "kableExtra"
) %>%
  row_spec(2, background = "#F5ABEA") %>%
  kable_styling()




# Create a function to check if terms are included in the formula
has_fe <- function(model, term) {
  term %in% attr(model$terms, "term.labels")
}

# Prepare the summary table
stargazer(models, 
          caption="Main results, different bandwidths (shortest distance between borders, not epicenters)",
          label="rdd_results_diffbandwidth-noEpi",
          type = "text", 
          column.labels = model_names, 
          covariate.labels = c("Treatment ZRR", "Distance to Frontier"), 
          omit = c("factor\\(dep\\)", controls), 
          add.lines = list(
            c("Controls", rep(TRUE, length(models))),  # Assuming controls are included in all models
            c("Department fixed effects", sapply(models, function(m) has_fe(m, "factor(dep)")))
          ), 
          omit.stat = c("LL", "ser", "f"), 
          #se = list(cluster_se, cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3), 
          star.cutoffs = c(0.05, 0.01, 0.001), 
          notes = "Standard errors are clustered at the canton level",
          out=paste0(path_tables, "tableC1.tex")
)



```


```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table D2: Main results when bandwidth is 10 km, different specifications
```{r }
if (file.exists(paste0(path_data, "script_sharp_noEpicenter.RData"))) {
  load(paste0(path_data, "script_sharp_noEpicenter.RData"))
} else {
  message("Data environment does not exist")
}

# Define bandwidth and threshold
bandwidth <- 20000  # Distance in meters
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```


```{r }

b <- 10000


# Initialize a list to store the regression models
models <- list()

df_rdd$log_pop_density <- log(df_rdd$pop / df_rdd$superficie)

# Define the groups of control variables
control_groups <- list(
  group1 = list(vars = c("x"), dep = FALSE, controls = FALSE),
  group2 = list(vars = c("x", "pop", "superficie"), dep = FALSE, controls = FALSE),
  group3 = list(vars = c("x", "pop", "superficie", "dep"), dep = TRUE, controls = FALSE),
  group4 = list(vars = c("x", "pop", "superficie", setdiff(controls, c("pop", "superficie"))), dep = FALSE, controls = TRUE),
  group6 = list(vars = c("x", "pop", "superficie", setdiff(controls, c("pop", "superficie")), "dep"), dep = TRUE, controls = TRUE)
)

df_filtered <- filter(df_rdd, x >= -b & x <= b)
df_filtered$x <- df_filtered$x / 1000

# Run regressions with grouped control variables and store the models
for (i in seq_along(control_groups)) {
  control_vars <- control_groups[[i]]$vars
  formula <- as.formula(paste("y ~ treatmentZRR +", paste(control_vars, collapse = " + ")))
  models[[paste0("group", i)]] <- lm(formula, data = df_filtered)
}

# Create lines indicating the inclusion of fixed effects and controls
fe_lines <- list(
  "Dept FE" = sapply(control_groups, function(x) ifelse(x$dep, "True", "False")),
  "Controls" = sapply(control_groups, function(x) ifelse(x$controls, "True", "False"))
)

# Generate the regression table using stargazer
stargazer(models, type = "text",
          #caption = "Main results when bandwidth is 10 km, different specifications (distance from border, not epicenter)",
          dep.var.labels = "FN vote share in 2002",
          covariate.labels = c("treatment ZRR", "Shortest distance from border"),
          omit = c(setdiff(controls, "superficie"), "dep"),  # Omit all control variables
          omit.stat = c("adj.rsq", "ser", "f"),
          add.lines = list(
            c("Controls", fe_lines$`Controls`),
            c("Dept FE", fe_lines$`Dept FE`)
          ),
          star.cutoffs = c(0.05, 0.01, 0.001), 
          column.sep.width = "3pt",
          float = FALSE,
          notes="I restrict the sample to the municipalities located 10km at most from the frontier program.",
          out=paste0(path_tables, "tableC2.tex"),
          tabel="rdd_results_10_noEpi")

```


```{r }
rm(list = setdiff(ls(), keep_vars))
```


## Table E1: Comparison of Residual Means between Control and Treatment Groups with T-Test Results
```{r }
if (file.exists(paste0(path_data, "borders_pair.RData"))) {
  load(paste0(path_data, "borders_pair.RData"))
} else {
  message("Data environment does not exist")
}

df_rct <- dfZRR1995Controls %>%
  select(-x) %>%
  rename(treatmentZRR=z)


ps_model <- matchit(treatmentZRR ~ pchom + FN1988 + delta_pop_1980_1995 + pop + ratEmp + ratForeigners + asso + educNoDiplomaPerK + educSUPPerK + educBACPerK + educCAPBEPPerK + poph + popf + pagri + pindp + ppint + pempl + pouvr + altitude + superficie + min_distance_to_agglo + logVac + haie + vigne + typologie, data = df_rct, method = "nearest", distance = "logit")

ps_model_refined <- matchit(treatmentZRR ~ pchom + FN1988 + delta_pop_1980_1995 + pop + ratEmp + ratForeigners + asso + educNoDiplomaPerK + educSUPPerK + educBACPerK + educCAPBEPPerK + poph + popf + pagri + pindp + ppint + pempl + pouvr + altitude + superficie + min_distance_to_agglo + logVac + haie + vigne + typologie, data = df_rct, method = "nearest", distance = "logit", caliper = 0.2)

df_rct <- match.data(ps_model_refined)
```
```{r }

labels <- c("FN1988"= "Vote share for FN in 1988",
            "pchom" = "Unemployed (%)", 
            "pop" = "Population", 
            "ratEmp" = "Employed (%)", 
            "ratForeigners" = "Foreigners (%)", 
            "asso" = "OPI per 1000 inhabitants", 
            "educNoDiplomaPerK" = "No diploma (%)", 
            "educSUPPerK" = "Academic education (%)", 
            "educBACPerK" = "Highschool education (%)", 
            "educCAPBEPPerK" = "Technical education (%)", 
            "poph" = "Proportion of 20-40, men", 
            "popf" = "Proportion of 20-40, women", 
            "pagri" = "Agriculture workers (%)", 
            "pindp" = "Independant workers (%)", 
            "ppint" = "Intermediate occupations (%)", 
            "pempl" = "Clerical workers (%)", 
            "pouvr" = "Manual workers (%)", 
            "altitude" = "Altitude", 
            "superficie" = "Locality size", 
            "logVac" = "Proportion of vacant housing (log)", 
            "haie" = "Fences per km2", 
            "vigne" = "Vines per km2", 
            "revenuPerK" = "Taxable income per capita",
            "delta_pop_1980_1995" = "Population change in p.p. 1982-1990", 
            "delta_emp_1980_1990" = "Employed change in p.p. 1982-1990", 
            "min_distance_to_agglo" = "Distance to closest agglomeration")

# Function to calculate mean and t-test of residuals
calculate_residual_balance <- function(df, grouping_var, target_vars, conditional_on) {
  # Create a list to store residuals for each target variable
  residuals_list <- list()
  
  # Calculate residuals for each target variable
  for (var in target_vars) {
    formula <- as.formula(paste(var, "~", paste(setdiff(conditional_on, var), collapse = " + ")))
    model <- lm(formula, data = df)
    residuals <- resid(model)
    residuals_list[[var]] <- residuals
    df[[paste0(var, "_resid")]] <- residuals
  }
  
  # Calculate means of residuals
  means <- df %>%
    group_by({{ grouping_var }}) %>%
    summarise(across(ends_with("_resid"), mean, na.rm = TRUE)) %>%
    pivot_longer(-{{ grouping_var }}, names_to = "variable", values_to = "mean") %>%
    mutate(variable = str_remove(variable, "_resid")) %>%
    pivot_wider(names_from = {{ grouping_var }}, values_from = mean, names_prefix = "mean_")
  
  # Calculate t-tests on residuals
  t_tests <- map_dfr(target_vars, function(var) {
    test <- t.test(df[[paste0(var, "_resid")]] ~ df[[rlang::as_string(rlang::ensym(grouping_var))]])
    data.frame(
      variable = var,
      estimate = test$estimate[2] - test$estimate[1],
      p.value = test$p.value
    )
  })
  
  # Merge means and t-tests
  summary <- means %>%
    left_join(t_tests, by = "variable") %>%
    mutate(significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ))
  
  return(summary)
}

# Define target variables
conditional_on <- c("dep", controls)

target_vars <- setdiff(controls, "typologie")

# Calculate balance summary using residuals
balance_summary <- calculate_residual_balance(df_rct, treatmentZRR, target_vars, conditional_on)

# Rename columns for better readability
balance_summary <- balance_summary %>%
  select(variable, `Control (mean)` = mean_FALSE, `Treatment (mean)` = mean_TRUE, `Difference` = estimate, `p-value` = p.value, `Significance` = significance)

# Replace variable names with labels
balance_summary$variable <- labels[balance_summary$variable]

# Round the values to 4 digits
balance_summary <- balance_summary %>%
  mutate(
    `Control (mean)` = round(`Control (mean)`, 4),
    `Treatment (mean)` = round(`Treatment (mean)`, 4),
    `Difference` = round(`Difference`, 4),
    `p-value` = round(`p-value`, 4)
  )

# Use stargazer to display the results in LaTeX format
stargazer(
  balance_summary %>%
    select(-Difference),  # Exclude the Difference column
  type = "latex",
  summary = FALSE,
  title = "Comparison of Residual Means between Control and Treatment Groups with T-Test Results, after matching",
  note = "The table displays the means of the residuals of the regression of the variable on the department fixed effects along with the other set of controls. The right columns show the significance of the t-test to compare both groups among the border municipalities. The sample corresponds to the matched municipalities.",
  digits = 2,
  dep.var.labels.include = FALSE,
  rownames = FALSE,
  label="tab:ttest-border",
  out=paste0(path_tables, "ttest_matching.tex")
)


```


```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table E2: Border municipalities: regression Results (matching)
```{r }
if (file.exists(paste0(path_data, "borders_pair.RData"))) {
  load(paste0(path_data, "borders_pair.RData"))
} else {
  message("Data environment does not exist")
}

df_rct <- dfZRR1995Controls %>%
  select(-x) %>%
  rename(treatmentZRR=z)


ps_model <- matchit(treatmentZRR ~ pchom + FN1988 + delta_pop_1980_1995 + pop + ratEmp + ratForeigners + asso + educNoDiplomaPerK + educSUPPerK + educBACPerK + educCAPBEPPerK + poph + popf + pagri + pindp + ppint + pempl + pouvr + altitude + superficie + min_distance_to_agglo + logVac + haie + vigne + typologie, data = df_rct, method = "nearest", distance = "logit")

ps_model_refined <- matchit(treatmentZRR ~ pchom + FN1988 + delta_pop_1980_1995 + pop + ratEmp + ratForeigners + asso + educNoDiplomaPerK + educSUPPerK + educBACPerK + educCAPBEPPerK + poph + popf + pagri + pindp + ppint + pempl + pouvr + altitude + superficie + min_distance_to_agglo + logVac + haie + vigne + typologie, 
                            data = df_rct, 
                            method = "nearest", # optimal, nearest
                            #distance = "logit", 
                            caliper = 0.001) # 0.2

df_rct <- match.data(ps_model_refined)
```
```{r }

# 1
model1 <- lm(y ~ treatmentZRR, data = df_rct)

# 2
formula <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + ")))
model2 <- lm(formula, data = df_rct)

# 3
formula <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + "), "+ factor(dep)"))
model3 <- lm(formula, data = df_rct)

# 4
formula <- as.formula(paste("y ~ treatmentZRR +", paste(controls, collapse = " + "), " + factor(border_pair) + factor(dep)"))
model4 <- lm(formula, data = df_rct)



# Use stargazer to display the results with a note about fixed effects
stargazer(
  model1, model2, model3, model4,
  type = "latex",
  title = "Regression Results",
  star.cutoffs = c(0.05, 0.01, 0.001), 
  label="tab:border-results",
  omit = c( "factor\\(dep\\)", "factor\\(border_pair\\)", controls),
    dep.var.labels = "The vote share for FN in 2002",
  add.lines = list(
    c("Controls", "No", "Yes", "Yes", "Yes"),
    c("Department Fixed Effects", "No", "No", "Yes", "No"),
    c("Border pair Fixed Effects", "No", "No", "No", "Yes")
  ),
  digits = 3,
  notes="The table presents the regression results of the effect of the ZRR program on the vote share for the FN in 2002, using a sample of 11,604 pairs of border municipalities that are in the same department. Specification (1) shows the simplest model with no controls. Specification (2) includes control variables. Specification (3) adds department fixed effects to the model. Specification (4) includes border pair fixed effects.",
  out = paste0(path_tables, "tableD2.tex")
)

```


```{r }
rm(list = setdiff(ls(), keep_vars))
```


## Table F1: Main results when bandwidth is 10 km, different specifications, 1995
```{r }
if (file.exists(paste0(path_data, "script_sharp_1995.RData"))) {
  load(paste0(path_data, "script_sharp_1995.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0

# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```

```{r }

b <- 10000


# Initialize a list to store the regression models
models <- list()

df_rdd$log_pop_density <- log(df_rdd$pop / df_rdd$superficie)

# Define the groups of control variables
control_groups <- list(
  group1 = list(vars = c("x"), dep = FALSE, controls = FALSE),
  group2 = list(vars = c("x", "pop", "superficie"), dep = FALSE, controls = FALSE),
  group3 = list(vars = c("x", "pop", "superficie", "dep"), dep = TRUE, controls = FALSE),
  group4 = list(vars = c("x", "pop", "superficie", setdiff(controls, c("pop", "superficie"))), dep = FALSE, controls = TRUE),
  group6 = list(vars = c("x", "pop", "superficie", setdiff(controls, c("pop", "superficie")), "dep"), dep = TRUE, controls = TRUE)
)

df_filtered <- filter(df_rdd, x >= -b & x <= b)
df_filtered$x <- df_filtered$x / 1000

# Run regressions with grouped control variables and store the models
for (i in seq_along(control_groups)) {
  control_vars <- control_groups[[i]]$vars
  formula <- as.formula(paste("y ~ treatmentZRR +", paste(control_vars, collapse = " + ")))
  models[[paste0("group", i)]] <- lm(formula, data = df_filtered)
}

# Create lines indicating the inclusion of fixed effects and controls
fe_lines <- list(
  "Dept FE" = sapply(control_groups, function(x) ifelse(x$dep, "True", "False")),
  "Controls" = sapply(control_groups, function(x) ifelse(x$controls, "True", "False"))
)

# Generate the regression table using stargazer
stargazer(models, type = "text",
          title = "Main results when bandwidth is 10 km, different specifications",
          dep.var.labels = "y",
          covariate.labels = c("treatmentZRR", "x"),
          omit = c(setdiff(controls, "superficie"), "dep"),  # Omit all control variables
          omit.stat = c("adj.rsq", "ser", "f"),
          add.lines = list(
            c("Controls", fe_lines$`Controls`),
            c("Dept FE", fe_lines$`Dept FE`)
          ),
          star.cutoffs = c(0.05, 0.01, 0.001), 
          column.sep.width = "3pt",
          float = FALSE,
          notes="I restrict the sample to the municipalities located 10km at most from the frontier program.",
          out=paste0(path_tables, "tableF1.tex"),
          tabel="tab:rdd_results_10")

```






```{r }
rm(list = setdiff(ls(), keep_vars))
```

## Table F2: Main results, different bandwidths
```{r }
if (file.exists(paste0(path_data, "script_sharp_1995.RData"))) {
  load(paste0(path_data, "script_sharp_1995.RData"))
} else {
  message("Data environment does not exist")
}

bandwidth <- 20000 # it corresponds to the distance (in meters)
threshold <- 0


# Filter the data within the specified bandwidth
df_rdd <- dfZRR1995Controls %>%
  filter(x >= (threshold - bandwidth) & x <= (threshold + bandwidth)) %>%
  mutate(
    dist = x,                     # Create a distance variable
    treatmentZRR = z,              # Define treatment indicator
    pop = log(pop)                 # Log-transform population
  ) %>%
  distinct(codecommune, .keep_all = TRUE)  # Remove duplicate communes

# Load and prepare canton data
dfCanton <- read_csv("/Users/ilanpargamin/Desktop/elections_papers/DATA/socio_eco/Taille_agglo_commune_csv/codescommunescantons1999.csv", show_col_types = FALSE) %>%
  select(codecommune, codecanton, dep) %>%
  filter(!is.na(codecanton) & !is.na(codecommune)) %>%
  mutate(codecommune = gsub("^0+", "", codecommune))  # Remove leading zeros from codecommune

# Merge df_rdd with canton data on codecommune
df_rdd <- df_rdd %>%
  select(-dep) %>%                      # Remove dep column from df_rdd before merging
  left_join(dfCanton, by = "codecommune") %>%  # Merge to add canton data
  mutate(canton = as.character(codecanton)) %>% # Convert codecanton to character as canton
  select(-codecanton)                    # Remove codecanton after renaming

# Clean up workspace
rm(bandwidth, threshold, dfCanton)

```
```{r }
b1 <- 20000
b2 <- 10000
b3 <- 5000

df_rdd$x_km <- df_rdd$x / 1000

formula <- as.formula(paste("y ~ z + x_km + ", paste(controls, collapse = " + "), "+ factor(dep) "))


model_bw_1 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b1 &
                                 x <= b1))

cluster_se_bw_1 <- coeftest(model_bw_1, vcov = vcovHC(model_bw_1, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))


model_bw_2 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b2 &
                                 x <= b2))

cluster_se_bw_2 <- coeftest(model_bw_2, vcov = vcovHC(model_bw_2, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))

model_bw_3 <- lm(formula,
                 data = filter(df_rdd,
                               x >= -b3 &
                                 x <= b3))

cluster_se_bw_3 <- coeftest(model_bw_3, vcov = vcovHC(model_bw_3, type = "HC1", cluster = "group", cluster.id = df_rdd$canton))


model_names <- c(paste0("Bandwidth = ", b1), 
                 paste0("Bandwidth = ", b2), 
                 paste0("Bandwidth = ", b3))
models <- list( model_bw_1, model_bw_2, model_bw_3)

names(models) <- model_names


modelsummary(models, 
             vcov = list(cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3),  # Use the clustered standard errors
                          estimate = "{estimate} ({std.error}){stars}",  # Correct formatting
             stars = c("*" = 0.05, "**" = 0.01, "***" = 0.001),  # Customize significance levels if needed
              notes = c("Standard errors are clustered at the canton level"),  # Optional explanatory note
             statistic = c(),
             gof_omit = "IC|Log|Adj|p\\.value|statistic|se_type|Std.Errors|RMSE",
             output = "kableExtra"
) %>%
  row_spec(2, background = "#F5ABEA") %>%
  kable_styling()



# Create a function to check if terms are included in the formula
has_fe <- function(model, term) {
  term %in% attr(model$terms, "term.labels")
}

# Prepare the summary table
stargazer(models, 
          type = "latex", 
          column.labels = model_names, 
          covariate.labels = c("Treatment ZRR", "Distance to Frontier"), 
          omit = c("factor\\(dep\\)", controls), 
          add.lines = list(
            c("Controls", rep(TRUE, length(models))),  # Assuming controls are included in all models
            c("Department fixed effects", sapply(models, function(m) has_fe(m, "factor(dep)")))
          ), 
          omit.stat = c("LL", "ser", "f"), 
          #se = list(cluster_se, cluster_se_bw_1, cluster_se_bw_2, cluster_se_bw_3), 
          star.cutoffs = c(0.05, 0.01, 0.001), 
          notes = "Standard errors are clustered at the canton level",
          out=paste0(path_tables, "tableF2.tex"),
          title="Main results, different bandwidths (1995)",
          label="tab:rdd_results_diffbandwidth-1995"
)



```




```{r }
rm(list = setdiff(ls(), keep_vars))
```


